{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQu8rYMNWy6k",
        "outputId": "9d1082f2-0f63-450f-c98c-8a225fcf25c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.1\n",
            "Collecting gymnasium[box2d]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[box2d])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.0)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376104 sha256=b36ffb8cd6c2afd37565f17725a04ff46307dad8b2c87977f50d51a7d4f34670\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: farama-notifications, box2d-py, gymnasium\n",
            "Successfully installed box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install swig\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install torch\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW9ldzKwTyVz",
        "outputId": "506e39f4-907a-410a-e10f-3141dc6f417f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1/1000, Total Reward: -116.55825413929149, Elapsed Time: 1.6401941776275635\n",
            "Episode 2/1000, Total Reward: -113.77139864167385, Elapsed Time: 0.5719234943389893\n",
            "Episode 3/1000, Total Reward: -200.02373345503477, Elapsed Time: 28.230777263641357\n",
            "Episode 4/1000, Total Reward: -117.82366079963049, Elapsed Time: 0.5650448799133301\n",
            "Episode 5/1000, Total Reward: -102.69551726115358, Elapsed Time: 0.6451592445373535\n",
            "Episode 6/1000, Total Reward: -200.0860055646754, Elapsed Time: 23.724088191986084\n",
            "Episode 7/1000, Total Reward: -115.56721382040965, Elapsed Time: 0.39601922035217285\n",
            "Episode 8/1000, Total Reward: -102.35256467714471, Elapsed Time: 0.49587011337280273\n",
            "Episode 9/1000, Total Reward: -104.71487020997355, Elapsed Time: 0.5037298202514648\n",
            "Episode 10/1000, Total Reward: -200.06902990419175, Elapsed Time: 22.588939905166626\n",
            "Episode 11/1000, Total Reward: -109.95608196851339, Elapsed Time: 1.104276418685913\n",
            "Episode 12/1000, Total Reward: -98.8118908357424, Elapsed Time: 1.1015269756317139\n",
            "Episode 13/1000, Total Reward: -103.97693593971916, Elapsed Time: 0.5140931606292725\n",
            "Episode 14/1000, Total Reward: -113.9442828190693, Elapsed Time: 0.38846683502197266\n",
            "Episode 15/1000, Total Reward: -200.06810339029155, Elapsed Time: 24.978315830230713\n",
            "Episode 16/1000, Total Reward: -103.34066430154431, Elapsed Time: 0.876798152923584\n",
            "Episode 17/1000, Total Reward: -98.70754528477701, Elapsed Time: 0.4501345157623291\n",
            "Episode 18/1000, Total Reward: -104.9315892453899, Elapsed Time: 0.47087812423706055\n",
            "Episode 19/1000, Total Reward: -200.13719804746387, Elapsed Time: 24.938409328460693\n",
            "Episode 20/1000, Total Reward: -101.6659344440588, Elapsed Time: 0.5938799381256104\n",
            "Episode 21/1000, Total Reward: -200.0266568120323, Elapsed Time: 24.369691848754883\n",
            "Episode 22/1000, Total Reward: -108.383846157241, Elapsed Time: 0.38976216316223145\n",
            "Episode 23/1000, Total Reward: -200.04757504805542, Elapsed Time: 24.013556241989136\n",
            "Episode 24/1000, Total Reward: -112.3059285236191, Elapsed Time: 0.37751102447509766\n",
            "Episode 25/1000, Total Reward: -114.17983351689813, Elapsed Time: 0.45514917373657227\n",
            "Episode 26/1000, Total Reward: -98.5428282664042, Elapsed Time: 0.3738367557525635\n",
            "Episode 27/1000, Total Reward: -200.0577364508146, Elapsed Time: 22.026087999343872\n",
            "Episode 28/1000, Total Reward: -99.14218184276162, Elapsed Time: 0.5916945934295654\n",
            "Episode 29/1000, Total Reward: -200.0573043947205, Elapsed Time: 25.487097024917603\n",
            "Episode 30/1000, Total Reward: -200.00029068975357, Elapsed Time: 23.98760747909546\n",
            "Episode 31/1000, Total Reward: -200.06498483672783, Elapsed Time: 21.50878643989563\n",
            "Episode 32/1000, Total Reward: -119.61038732411377, Elapsed Time: 0.7442822456359863\n",
            "Episode 33/1000, Total Reward: -100.63946318775031, Elapsed Time: 0.6088919639587402\n",
            "Episode 34/1000, Total Reward: -118.98958814083699, Elapsed Time: 0.41891050338745117\n",
            "Episode 35/1000, Total Reward: -102.85428827438454, Elapsed Time: 0.44683194160461426\n",
            "Episode 36/1000, Total Reward: -101.76237684637383, Elapsed Time: 0.5704696178436279\n",
            "Episode 37/1000, Total Reward: -110.7764786572028, Elapsed Time: 0.39640188217163086\n",
            "Episode 38/1000, Total Reward: -200.08541287796072, Elapsed Time: 24.44418954849243\n",
            "Episode 39/1000, Total Reward: -200.12486011990063, Elapsed Time: 24.34429621696472\n",
            "Episode 40/1000, Total Reward: -123.43924573991336, Elapsed Time: 1.1044738292694092\n",
            "Episode 41/1000, Total Reward: -200.01018222671104, Elapsed Time: 23.4825382232666\n",
            "Episode 42/1000, Total Reward: -200.05269071380596, Elapsed Time: 24.200556755065918\n",
            "Episode 43/1000, Total Reward: -98.14157059594517, Elapsed Time: 0.4610297679901123\n",
            "Episode 44/1000, Total Reward: -110.49758757543935, Elapsed Time: 0.35245800018310547\n",
            "Episode 45/1000, Total Reward: -102.59520025357467, Elapsed Time: 0.6631300449371338\n",
            "Episode 46/1000, Total Reward: -113.1171725267784, Elapsed Time: 0.6160130500793457\n",
            "Episode 47/1000, Total Reward: -200.2540341004814, Elapsed Time: 24.63363552093506\n",
            "Episode 48/1000, Total Reward: -200.03899732173397, Elapsed Time: 26.376482009887695\n",
            "Episode 49/1000, Total Reward: -102.35467059359314, Elapsed Time: 0.5252234935760498\n",
            "Episode 50/1000, Total Reward: -100.67397162767537, Elapsed Time: 0.426196813583374\n",
            "Episode 51/1000, Total Reward: -111.25885749007948, Elapsed Time: 0.4882197380065918\n",
            "Episode 52/1000, Total Reward: -101.75972087338405, Elapsed Time: 0.2862560749053955\n",
            "Episode 53/1000, Total Reward: -200.01774635962622, Elapsed Time: 25.166897773742676\n",
            "Episode 54/1000, Total Reward: -200.04634407210273, Elapsed Time: 25.880724668502808\n",
            "Episode 55/1000, Total Reward: -100.16379885740032, Elapsed Time: 0.5361568927764893\n",
            "Episode 56/1000, Total Reward: -200.0199147675753, Elapsed Time: 24.77731204032898\n"
          ]
        }
      ],
      "source": [
        "# Definizione dell'ambiente\n",
        "ENV = \"BipedalWalker-v3\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "#agent\n",
        "ACT_BUCKETS = 11\n",
        "EPISODES = 1000\n",
        "REWARD_THRESHOLD = -200\n",
        "GAMMA = 0.9\n",
        "ALPHA = 0.01\n",
        "EPSILON_INIT = 1.0\n",
        "EPSILON_DECAY = 0.999\n",
        "EPSILON_MIN = 0.05\n",
        "NORMALIZE = True\n",
        "\n",
        "#experience replay\n",
        "BATCH_SIZE = 64\n",
        "MEM_SIZE = 1000000\n",
        "\n",
        "#neural network\n",
        "HIDDEN_SIZE = 512\n",
        "LR = 1e-3\n",
        "\n",
        "# Experience Replay\n",
        "class ExperienceReplay:\n",
        "    def __init__(self, buffer_size, batch_size=BATCH_SIZE):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def store_transition(self, state, action, reward, new_state, done):\n",
        "        self.buffer.append((state, action, reward, new_state, done))\n",
        "\n",
        "    def sample(self):\n",
        "        sample = random.sample(self.buffer, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*sample)\n",
        "\n",
        "        # stack: turns a list of tensors into a tensor with a higher dimension\n",
        "        states = torch.stack(states).to(DEVICE)\n",
        "        next_states = torch.stack(next_states).to(DEVICE)\n",
        "\n",
        "        # tensor: converts a list of values into a tensor\n",
        "        actions = torch.tensor(actions).to(DEVICE)\n",
        "        rewards = torch.tensor(rewards).float().to(DEVICE)\n",
        "        dones = torch.tensor(dones).short().to(DEVICE)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=HIDDEN_SIZE):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class Normalizer:\n",
        "    def __init__(self, num_inputs):\n",
        "        self.mean = np.zeros(num_inputs)\n",
        "        self.m2 = np.zeros(num_inputs)\n",
        "        self.count = 0\n",
        "\n",
        "    # Welford's online algorithm for update using unbiased variance\n",
        "    # more info: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm\n",
        "    def update(self, x):\n",
        "        self.count += 1\n",
        "        old_mean = self.mean.copy()\n",
        "        self.mean += (x - self.mean) / self.count\n",
        "        self.m2 += (x - old_mean) * (x - self.mean)\n",
        "\n",
        "    def normalize(self, x):\n",
        "        eps = 1e-10\n",
        "        mean = torch.tensor(self.mean).float().to(DEVICE)\n",
        "        if self.count > 1:\n",
        "            variance = self.m2 / (self.count - 1)\n",
        "        else:\n",
        "            variance = np.zeros_like(self.m2)\n",
        "        stdev = torch.tensor(np.sqrt(variance) + eps).float().to(DEVICE)\n",
        "        x = (x - mean) / (stdev)\n",
        "        return x\n",
        "\n",
        "# Agent\n",
        "class Agent:\n",
        "    def __init__(self, env, episodes=EPISODES, gamma=GAMMA, alpha=ALPHA, epsilon_init=EPSILON_INIT, epsilon_decay=EPSILON_DECAY, epsilon_min=EPSILON_MIN,\n",
        "                 experience_replay_size=MEM_SIZE, act_buckets=ACT_BUCKETS, reward_threshold=REWARD_THRESHOLD, normalize=NORMALIZE, lr=LR, render=False):\n",
        "        self.env = env\n",
        "        self.episodes = episodes\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon_init\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.memory = ExperienceReplay(experience_replay_size)\n",
        "        self.action_buckets = act_buckets\n",
        "        self.reward_threshold = reward_threshold\n",
        "        self.render = render\n",
        "        self.render_interval = 10\n",
        "\n",
        "        self.model = QNetwork(env.observation_space.shape[0], self.action_buckets**env.action_space.shape[0]).to(DEVICE)\n",
        "        # train the NN every \"learning_frequency\" steps\n",
        "        self.learning_frequency = 1\n",
        "        # weight_decay is the L2 regularization parameter in Adam\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        self.normalize = normalize\n",
        "        # dynamic normalization computing mean and variance based on observations\n",
        "        if normalize:\n",
        "            self.normalizer = Normalizer(env.observation_space.shape[0])\n",
        "\n",
        "    def discretize_action(self, action):\n",
        "        discrete_action = np.round((action - self.env.action_space.low) / (self.env.action_space.high - self.env.action_space.low) * (self.action_buckets - 1)).astype(int)\n",
        "        return tuple(discrete_action)\n",
        "\n",
        "    def undiscretize_action(self, discrete_action):\n",
        "        action = (discrete_action / (self.action_buckets - 1)) * (self.env.action_space.high - self.env.action_space.low) + self.env.action_space.low\n",
        "        return tuple(action)\n",
        "\n",
        "    def store(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "        if len(self.memory) > BATCH_SIZE:\n",
        "            self.learn()\n",
        "\n",
        "    def updateDQN(self):\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample()\n",
        "        if self.normalize:\n",
        "            states = self.normalizer.normalize(states)\n",
        "            next_states = self.normalizer.normalize(next_states)\n",
        "        q_eval = self.model(states)\n",
        "        q_next = self.model(next_states)\n",
        "\n",
        "        # takes the q_value corresponding to the chosen action (for each sample)\n",
        "        q_eval_actions = q_eval.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        q_target = q_eval_actions * (1 - self.alpha) + self.alpha * (rewards + self.gamma * q_next.max(1)[0] * (1 - dones))\n",
        "\n",
        "        loss = F.mse_loss(q_eval_actions, q_target)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            discrete_action = np.random.randint(0, self.action_buckets**self.env.action_space.shape[0])\n",
        "            return discrete_action\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state = torch.tensor(state).float().to(DEVICE)\n",
        "                if self.normalize:\n",
        "                    state = self.normalizer.normalize(state)\n",
        "                q_values = self.model(state)\n",
        "                discrete_action = q_values.argmax().item()\n",
        "\n",
        "                return discrete_action\n",
        "\n",
        "    def learn(self):\n",
        "        rewards = []\n",
        "        elapsed_times = []\n",
        "        losses = []\n",
        "\n",
        "        for episode in range(1, self.episodes + 1):\n",
        "            total_reward = 0\n",
        "            steps_taken = 0\n",
        "            start_time = time.time()\n",
        "            episode_losses = []\n",
        "\n",
        "            observation = self.env.reset()[0]\n",
        "            if self.normalize:\n",
        "                self.normalizer.update(observation)\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "            while total_reward > self.reward_threshold:\n",
        "                # action is now a number between 0 and act_buckets ^ action_space_size\n",
        "                action = self.choose_action(observation)\n",
        "                # map the number to a 4-dimensional array\n",
        "                discrete_action = np.array(np.unravel_index(action, [self.action_buckets] * self.env.action_space.shape[0]))\n",
        "                # extract the corresponding continuous action\n",
        "                continuous_action = self.undiscretize_action(discrete_action)\n",
        "\n",
        "                next_observation, reward, done, _, _ = self.env.step(continuous_action)\n",
        "                if self.normalize:\n",
        "                    self.normalizer.update(next_observation)\n",
        "\n",
        "                self.memory.store_transition(torch.tensor(observation).float().to(DEVICE), torch.tensor(action).long().to(DEVICE),\n",
        "                                             reward, torch.tensor(next_observation).float().to(DEVICE), done)\n",
        "\n",
        "                if steps_taken % self.learning_frequency == 0 and len(self.memory) > self.memory.batch_size:\n",
        "                    current_loss = self.updateDQN()\n",
        "                    episode_losses.append(current_loss)\n",
        "\n",
        "                if self.render and steps_taken % self.render_interval == 0:\n",
        "                    self.env.render()\n",
        "\n",
        "                total_reward += reward\n",
        "                observation = next_observation\n",
        "                steps_taken += 1\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "            elapsed_times.append(elapsed_time)\n",
        "            losses.append(np.mean(episode_losses))\n",
        "            rewards.append(total_reward)  # Store total reward for this episode\n",
        "            print(f\"Episode {episode}/{self.episodes}, Total Reward: {total_reward}, Elapsed Time: {elapsed_time}\")\n",
        "\n",
        "        self.env.close()\n",
        "        max_reward = max(rewards)  # Calculate the maximum reward\n",
        "        print(f\"Maximum Reward: {max_reward}\")\n",
        "        return self.model.state_dict(), rewards, losses, elapsed_times\n",
        "\n",
        "env = gym.make(ENV)\n",
        "agent = Agent(env)\n",
        "model_params, rewards, losses, elapsed_times = agent.learn()\n",
        "\n",
        "# Save the trained model parameters\n",
        "torch.save(model_params, 'dqn_model.pth')\n",
        "\n",
        "window_size = 100\n",
        "# Function to calculate the moving average using np.mean\n",
        "def moving_average(data, window_size=window_size):\n",
        "    moving_averages = []\n",
        "    for i in range(len(data) - window_size + 1):\n",
        "        window = data[i:i + window_size]\n",
        "        window_average = np.mean(window)\n",
        "        moving_averages.append(window_average)\n",
        "    return moving_averages\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(rewards, label='Total Reward')\n",
        "if len(rewards) >= window_size:\n",
        "    plt.plot(range(window_size, len(rewards) + 1), moving_average(rewards), label='Moving Average (100 episodes)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title(f'Reward Evolution (DQN)')\n",
        "plt.legend()\n",
        "plt.savefig(f'Rewards Evolutions DQN.png')\n",
        "\n",
        "# Plot losses\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(losses, label=f'Losses (DQN)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.title(f'Loss Value per Episode')\n",
        "plt.legend()\n",
        "plt.savefig(f'Losses DQN.png')\n",
        "\n",
        "# Plot elapsed times\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(elapsed_times, label=f'Elapsed Time (DQN)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Elapsed Time (s)')\n",
        "plt.title(f'Elapsed Time per Episode')\n",
        "plt.legend()\n",
        "plt.savefig(f'Elapsed Times DQN.png')\n",
        "\n",
        "time.sleep(3600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m45TsNgFoG07"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Definizione dell'ambiente\n",
        "ENV = \"BipedalWalker-v3\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPISODES = 100\n",
        "NORMALIZE = True\n",
        "\n",
        "\n",
        "class Normalizer:\n",
        "    def __init__(self, num_inputs):\n",
        "        self.mean = np.zeros(num_inputs)\n",
        "        self.m2 = np.zeros(num_inputs)\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, x):\n",
        "        self.count += 1\n",
        "        old_mean = self.mean.copy()\n",
        "        self.mean += (x - self.mean) / self.count\n",
        "        self.m2 += (x - old_mean) * (x - self.mean)\n",
        "\n",
        "    def normalize(self, x):\n",
        "        eps = 1e-10\n",
        "        mean = torch.tensor(self.mean).float().to(DEVICE)\n",
        "        if self.count > 1:\n",
        "            variance = self.m2 / (self.count - 1)\n",
        "        else:\n",
        "            variance = np.zeros_like(self.m2)\n",
        "        stdev = torch.tensor(np.sqrt(variance) + eps).float().to(DEVICE)\n",
        "        x = (x - mean) / (stdev)\n",
        "        return x\n",
        "\n",
        "\n",
        "action_buckets = 11\n",
        "\n",
        "# Carica i parametri del modello salvato\n",
        "model_params = torch.load('dqn_model.pth', map_location=DEVICE)\n",
        "\n",
        "# Crea l'ambiente\n",
        "env = gym.make(ENV)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "\n",
        "# Crea il modello e carica i parametri\n",
        "model = QNetwork(state_dim, action_buckets ** action_dim).to(DEVICE)\n",
        "model.load_state_dict(model_params)\n",
        "model.eval()  # Imposta il modello in modalità valutazione\n",
        "\n",
        "def test_DQN(dqn, env, episodes, normalize):\n",
        "    rewards = []\n",
        "    if normalize:\n",
        "        normalizer = Normalizer(env.observation_space.shape[0])\n",
        "    for i in range(episodes):\n",
        "        observation = env.reset()[0]\n",
        "        if normalize:\n",
        "            normalizer.update(observation)\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            with torch.no_grad():  # Disabilita il calcolo dei gradienti durante l'inferenza\n",
        "                # Preprocessa l'osservazione se necessario\n",
        "                state = torch.tensor(observation).float().to(DEVICE)\n",
        "                if normalize:\n",
        "                    state = normalizer.normalize(state)\n",
        "                q_values = dqn(state)\n",
        "\n",
        "                # Scegli l'azione con il massimo Q-value\n",
        "                flat_discrete_action = q_values.argmax().item()\n",
        "                discrete_action = np.array(np.unravel_index(flat_discrete_action, [action_buckets] * env.action_space.shape[0]))\n",
        "                action = (discrete_action / (action_buckets - 1)) * (env.action_space.high - env.action_space.low) + env.action_space.low\n",
        "\n",
        "            # Esegui l'azione nell'ambiente\n",
        "            next_observation, reward, done, _, _ = env.step(action)\n",
        "            if normalize:\n",
        "                normalizer.update(next_observation)\n",
        "            total_reward += reward\n",
        "            observation = next_observation\n",
        "\n",
        "            # Visualizza l'ambiente (commenta questa linea se non vuoi visualizzare l'ambiente)\n",
        "            env.render()\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        print(f\"Episode {i + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "    env.close()  # Assicurati di chiudere l'ambiente\n",
        "    print(\"Testing completed.\")\n",
        "    return rewards\n",
        "\n",
        "# Esegui il test\n",
        "rewards = test_DQN(model, env, EPISODES, NORMALIZE)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(rewards, label='Total Reward')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title(f'Rewards of DQN (Testing with Loaded network)')\n",
        "plt.legend()\n",
        "plt.savefig(f'Rewards DQN.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAsB2PNILQJ_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "#test normalizer\n",
        "\n",
        "class Normalizer:\n",
        "    def __init__(self, num_inputs):\n",
        "        self.mean = np.zeros(num_inputs)\n",
        "        self.m2 = np.zeros(num_inputs)\n",
        "        self.count = 0\n",
        "\n",
        "    # Welford's online algorithm for update using unbiased variance\n",
        "    # more info: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm\n",
        "    def update(self, x):\n",
        "        self.count += 1\n",
        "        old_mean = self.mean.copy()\n",
        "        self.mean += (x - self.mean) / self.count\n",
        "        self.m2 += (x - old_mean) * (x - self.mean)\n",
        "\n",
        "    def normalize(self, x):\n",
        "        print(\"Input:\", x)\n",
        "        eps = 1e-10\n",
        "        mean = torch.tensor(self.mean).float().to(DEVICE)\n",
        "        if self.count > 1:\n",
        "            variance = self.m2 / (self.count - 1)\n",
        "        else:\n",
        "            variance = np.zeros_like(self.m2)\n",
        "        stdev = torch.tensor(np.sqrt(variance) + eps).float().to(DEVICE)\n",
        "        x = (x - mean) / (stdev)\n",
        "        print(\"Normalized:\", x)\n",
        "        print(\"Mean:\", mean)\n",
        "        print(\"Std Dev:\", stdev)\n",
        "        return x\n",
        "\n",
        "# Esempio di utilizzo\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Creare un'istanza di Normalizer con 24 dimensioni\n",
        "normalizer = Normalizer(num_inputs=24)\n",
        "\n",
        "# Generare alcuni campioni casuali e aggiornare il normalizzatore\n",
        "for _ in range(10):\n",
        "    data = np.random.randn(24)\n",
        "    normalizer.update(data)\n",
        "    data = torch.tensor(data).float().to(DEVICE)\n",
        "    normalized_data = normalizer.normalize(data)\n",
        "\n",
        "# Generare un nuovo campione casuale e normalizzarlo\n",
        "test_data = torch.tensor(np.random.randn(24)).float().to(DEVICE)\n",
        "normalized_data = normalizer.normalize(test_data)\n",
        "print(normalized_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kueZQeZOHKtZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Supponendo che discrete_action sia l'indice singolo che rappresenta l'azione discreta\n",
        "discrete_action = 123  # Esempio di indice\n",
        "\n",
        "# Calcoliamo gli indici per ogni dimensione delle azioni\n",
        "action_indices = np.unravel_index(discrete_action, [action_buckets] * env.action_space.shape[0])\n",
        "\n",
        "# Stampa degli indici per debug\n",
        "print(\"Indices for each action dimension:\", action_indices)\n",
        "\n",
        "# Se vuoi convertire questi indici in una tupla di azione discreta\n",
        "discrete_action_tuple = tuple(action_indices)\n",
        "\n",
        "# Stampa dell'azione discreta come tupla\n",
        "print(\"Discrete action as tuple:\", discrete_action_tuple)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
