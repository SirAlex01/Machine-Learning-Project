{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install swig\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install torch\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "dQu8rYMNWy6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e71ed81e-dcc1-4907-9b2c-02ba51673b2b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.2.1)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definizione dell'ambiente\n",
        "ENV = \"BipedalWalker-v3\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "#agent\n",
        "ACT_BUCKETS = 11\n",
        "EPISODES = 1000\n",
        "REWARD_THRESHOLD = -200\n",
        "GAMMA = 0.99\n",
        "ALPHA = 0.01\n",
        "EPSILON_INIT = 1.0\n",
        "EPSILON_DECAY = 0.997\n",
        "EPSILON_MIN = 0.05\n",
        "NORMALIZE = True\n",
        "\n",
        "#experience replay\n",
        "BATCH_SIZE = 16\n",
        "MEM_SIZE = 1000000\n",
        "\n",
        "#neural network\n",
        "HIDDEN_SIZE = 512\n",
        "LR = 1e-3\n",
        "L2_LAMBDA = 0.001\n",
        "\n",
        "# Experience Replay\n",
        "class ExperienceReplay:\n",
        "    def __init__(self, buffer_size, batch_size=BATCH_SIZE):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def store_transition(self, state, action, reward, new_state, done):\n",
        "        self.buffer.append((state, action, reward, new_state, done))\n",
        "\n",
        "    def sample(self):\n",
        "        sample = random.sample(self.buffer, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*sample)\n",
        "\n",
        "        # stack: turns a list of tensors into a tensor with a higher dimension\n",
        "        states = torch.stack(states).to(DEVICE)\n",
        "        next_states = torch.stack(next_states).to(DEVICE)\n",
        "\n",
        "        # tensor: converts a list of values into a tensor\n",
        "        actions = torch.tensor(actions).to(DEVICE)\n",
        "        rewards = torch.tensor(rewards).float().to(DEVICE)\n",
        "        dones = torch.tensor(dones).short().to(DEVICE)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=HIDDEN_SIZE):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class Normalizer:\n",
        "    def __init__(self, num_inputs):\n",
        "        self.mean = np.zeros(num_inputs)\n",
        "        self.m2 = np.zeros(num_inputs)\n",
        "        self.count = 0\n",
        "\n",
        "    # Welford's online algorithm for update using unbiased variance\n",
        "    # more info: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm\n",
        "    def update(self, x):\n",
        "        self.count += 1\n",
        "        old_mean = self.mean.copy()\n",
        "        self.mean += (x - self.mean) / self.count\n",
        "        self.m2 += (x - old_mean) * (x - self.mean)\n",
        "\n",
        "    def normalize(self, x):\n",
        "        eps = 1e-10\n",
        "        mean = torch.tensor(self.mean).float().to(DEVICE)\n",
        "        if self.count > 1:\n",
        "            variance = self.m2 / (self.count - 1)\n",
        "        else:\n",
        "            variance = np.zeros_like(self.m2)\n",
        "        stdev = torch.tensor(np.sqrt(variance) + eps).float().to(DEVICE)\n",
        "        x = (x - mean) / (stdev)\n",
        "        return x\n",
        "\n",
        "# Agent\n",
        "class Agent:\n",
        "    def __init__(self, env, episodes=EPISODES, gamma=GAMMA, alpha=ALPHA, epsilon_init=EPSILON_INIT, epsilon_decay=EPSILON_DECAY, epsilon_min=EPSILON_MIN,\n",
        "                 experience_replay_size=MEM_SIZE, act_buckets=ACT_BUCKETS, reward_threshold=REWARD_THRESHOLD, normalize=NORMALIZE, lr=LR, l2_lambda=L2_LAMBDA,\n",
        "                 render=False):\n",
        "        self.env = env\n",
        "        self.episodes = episodes\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon_init\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.memory = ExperienceReplay(experience_replay_size)\n",
        "        self.action_buckets = act_buckets\n",
        "        self.reward_threshold = reward_threshold\n",
        "        self.render = render\n",
        "        self.render_interval = 10\n",
        "\n",
        "        self.model = QNetwork(env.observation_space.shape[0], self.action_buckets**env.action_space.shape[0]).to(DEVICE)\n",
        "        # train the NN every \"learning_frequency\" steps\n",
        "        self.learning_frequency = 1\n",
        "        # weight_decay is the L2 regularization parameter in Adam\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=l2_lambda)\n",
        "\n",
        "        self.normalize = normalize\n",
        "        # dynamic normalization computing mean and variance based on observations\n",
        "        if normalize:\n",
        "            self.normalizer = Normalizer(env.observation_space.shape[0])\n",
        "\n",
        "    def discretize_action(self, action):\n",
        "        discrete_action = np.round((action - self.env.action_space.low) / (self.env.action_space.high - self.env.action_space.low) * (self.action_buckets - 1)).astype(int)\n",
        "        return tuple(discrete_action)\n",
        "\n",
        "    def undiscretize_action(self, discrete_action):\n",
        "        action = (discrete_action / (self.action_buckets - 1)) * (self.env.action_space.high - self.env.action_space.low) + self.env.action_space.low\n",
        "        return tuple(action)\n",
        "\n",
        "    def store(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "        if len(self.memory) > BATCH_SIZE:\n",
        "            self.learn()\n",
        "\n",
        "    def updateDQN(self):\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample()\n",
        "        if self.normalize:\n",
        "            states = self.normalizer.normalize(states)\n",
        "            next_states = self.normalizer.normalize(next_states)\n",
        "        q_eval = self.model(states)\n",
        "        q_next = self.model(next_states)\n",
        "\n",
        "        # takes the q_value corresponding to the chosen action (for each sample)\n",
        "        q_eval_actions = q_eval.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        q_target = q_eval_actions * (1 - self.alpha) + self.alpha * (rewards + self.gamma * q_next.max(1)[0] * (1 - dones))\n",
        "\n",
        "        loss = F.mse_loss(q_eval_actions, q_target)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            discrete_action = np.random.randint(0, self.action_buckets**self.env.action_space.shape[0])\n",
        "            return discrete_action\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state = torch.tensor(state).float().to(DEVICE)\n",
        "                if self.normalize:\n",
        "                    state = self.normalizer.normalize(state)\n",
        "                q_values = self.model(state)\n",
        "                discrete_action = q_values.argmax().item()\n",
        "                return discrete_action\n",
        "\n",
        "    def learn(self):\n",
        "        rewards = []\n",
        "        elapsed_times = []\n",
        "        for episode in range(1, self.episodes + 1):\n",
        "            total_reward = 0\n",
        "            steps_taken = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "            observation = self.env.reset()[0]\n",
        "            if self.normalize:\n",
        "                self.normalizer.update(observation)\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "            while total_reward > self.reward_threshold:\n",
        "                # action is now a number between 0 and act_buckets ^ action_space_size\n",
        "                action = self.choose_action(observation)\n",
        "                # map the number to a 4-dimensional array\n",
        "                discrete_action = np.array(np.unravel_index(action, [self.action_buckets] * self.env.action_space.shape[0]))\n",
        "                # extract the corresponding continuous action\n",
        "                continuous_action = self.undiscretize_action(discrete_action)\n",
        "\n",
        "                next_observation, reward, done, _, _ = self.env.step(continuous_action)\n",
        "                if self.normalize:\n",
        "                    self.normalizer.update(next_observation)\n",
        "\n",
        "                self.memory.store_transition(torch.tensor(observation).float().to(DEVICE), torch.tensor(action).long().to(DEVICE),\n",
        "                                             reward, torch.tensor(next_observation).float().to(DEVICE), done)\n",
        "\n",
        "                if steps_taken % self.learning_frequency == 0 and len(self.memory) > self.memory.batch_size:\n",
        "                    self.updateDQN()\n",
        "\n",
        "                if self.render and steps_taken % self.render_interval == 0:\n",
        "                    self.env.render()\n",
        "\n",
        "                total_reward += reward\n",
        "                observation = next_observation\n",
        "                steps_taken += 1\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "            elapsed_times.append(elapsed_time)\n",
        "            rewards.append(total_reward)  # Store total reward for this episode\n",
        "            print(f\"Episode {episode}/{self.episodes}, Total Reward: {total_reward}, Elapsed Time: {elapsed_time}\")\n",
        "\n",
        "        self.env.close()\n",
        "        max_reward = max(rewards)  # Calculate the maximum reward\n",
        "        print(f\"Maximum Reward: {max_reward}\")\n",
        "        return self.model.state_dict(), rewards, elapsed_times\n",
        "\n",
        "env = gym.make(ENV)\n",
        "agent = Agent(env)\n",
        "model_params, rewards, elapsed_times = agent.learn()\n",
        "\n",
        "# Save the trained model parameters\n",
        "torch.save(model_params, 'dqn_model.pth')\n",
        "\n",
        "window_size = 100\n",
        "# Function to calculate the moving average using np.mean\n",
        "def moving_average(data, window_size=window_size):\n",
        "    moving_averages = []\n",
        "    for i in range(len(data) - window_size + 1):\n",
        "        window = data[i:i + window_size]\n",
        "        window_average = np.mean(window)\n",
        "        moving_averages.append(window_average)\n",
        "    return moving_averages\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(rewards, label='Total Reward')\n",
        "if len(rewards) >= window_size:\n",
        "    plt.plot(range(window_size, len(rewards) + 1), moving_average(rewards), label='Moving Average (100 episodes)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title(f'Reward Evolution (DQN)')\n",
        "plt.legend()\n",
        "plt.savefig(f'Rewards Evolutions DQN.png')\n",
        "# Plot elapsed times\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(elapsed_times, label=f'Elapsed Time (DQN)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Elapsed Time (s)')\n",
        "plt.title(f'Elapsed Time per Episode')\n",
        "plt.legend()\n",
        "plt.savefig(f'Elapsed Times DQN.png')\n"
      ],
      "metadata": {
        "id": "BW9ldzKwTyVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19c26e60-1efa-4ab6-c525-1df88a513597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/1000, Total Reward: -200.0082658684871, Elapsed Time: 26.647910833358765\n",
            "Episode 2/1000, Total Reward: -120.69202096129196, Elapsed Time: 0.4352731704711914\n",
            "Episode 3/1000, Total Reward: -104.958879404669, Elapsed Time: 0.2887697219848633\n",
            "Episode 4/1000, Total Reward: -200.1703450685888, Elapsed Time: 21.27455735206604\n",
            "Episode 5/1000, Total Reward: -110.12498217522577, Elapsed Time: 0.4250340461730957\n",
            "Episode 6/1000, Total Reward: -101.73603273546385, Elapsed Time: 0.6423380374908447\n",
            "Episode 7/1000, Total Reward: -115.369302494021, Elapsed Time: 0.889523983001709\n",
            "Episode 8/1000, Total Reward: -117.77890840017784, Elapsed Time: 0.6981644630432129\n",
            "Episode 9/1000, Total Reward: -99.69895029945063, Elapsed Time: 0.6763782501220703\n",
            "Episode 10/1000, Total Reward: -113.4619227906232, Elapsed Time: 0.2875325679779053\n",
            "Episode 11/1000, Total Reward: -98.22350996879699, Elapsed Time: 0.4464852809906006\n",
            "Episode 12/1000, Total Reward: -100.26743503854995, Elapsed Time: 0.556718111038208\n",
            "Episode 13/1000, Total Reward: -200.03764570822773, Elapsed Time: 19.11384892463684\n",
            "Episode 14/1000, Total Reward: -160.7325043226037, Elapsed Time: 4.434007883071899\n",
            "Episode 15/1000, Total Reward: -99.75128796310251, Elapsed Time: 0.41644740104675293\n",
            "Episode 16/1000, Total Reward: -101.58322959749165, Elapsed Time: 0.5448319911956787\n",
            "Episode 17/1000, Total Reward: -99.40130688582559, Elapsed Time: 0.4425523281097412\n",
            "Episode 18/1000, Total Reward: -108.00130288241418, Elapsed Time: 0.33417630195617676\n",
            "Episode 19/1000, Total Reward: -101.78089036179458, Elapsed Time: 0.4024324417114258\n",
            "Episode 20/1000, Total Reward: -101.77050699085407, Elapsed Time: 0.4171881675720215\n",
            "Episode 21/1000, Total Reward: -110.39174527072895, Elapsed Time: 0.851224422454834\n",
            "Episode 22/1000, Total Reward: -119.59141082745752, Elapsed Time: 0.6714825630187988\n",
            "Episode 23/1000, Total Reward: -200.0115290436537, Elapsed Time: 19.43486452102661\n",
            "Episode 24/1000, Total Reward: -106.01362861187854, Elapsed Time: 0.24716448783874512\n",
            "Episode 25/1000, Total Reward: -200.09769861081804, Elapsed Time: 18.655710220336914\n",
            "Episode 26/1000, Total Reward: -200.03572708686207, Elapsed Time: 21.07215929031372\n",
            "Episode 27/1000, Total Reward: -200.01565752700677, Elapsed Time: 18.79920792579651\n",
            "Episode 28/1000, Total Reward: -200.15871172521454, Elapsed Time: 19.641493797302246\n",
            "Episode 29/1000, Total Reward: -101.962411451109, Elapsed Time: 0.3450126647949219\n",
            "Episode 30/1000, Total Reward: -95.02974188455492, Elapsed Time: 0.7582278251647949\n",
            "Episode 31/1000, Total Reward: -102.4715554737291, Elapsed Time: 0.3024120330810547\n",
            "Episode 32/1000, Total Reward: -105.27306860323114, Elapsed Time: 0.48274898529052734\n",
            "Episode 33/1000, Total Reward: -117.67737370953014, Elapsed Time: 0.31464695930480957\n",
            "Episode 34/1000, Total Reward: -107.19319339786607, Elapsed Time: 0.30694580078125\n",
            "Episode 35/1000, Total Reward: -200.09850178303884, Elapsed Time: 22.587286472320557\n",
            "Episode 36/1000, Total Reward: -101.89210439634981, Elapsed Time: 0.5384509563446045\n",
            "Episode 37/1000, Total Reward: -200.01148862935582, Elapsed Time: 22.81687307357788\n",
            "Episode 38/1000, Total Reward: -97.17539604841657, Elapsed Time: 0.6352143287658691\n",
            "Episode 39/1000, Total Reward: -200.145374003043, Elapsed Time: 19.200135946273804\n",
            "Episode 40/1000, Total Reward: -108.03631868353014, Elapsed Time: 0.4407675266265869\n",
            "Episode 41/1000, Total Reward: -200.00681960344002, Elapsed Time: 18.464072465896606\n",
            "Episode 42/1000, Total Reward: -264.9455013937476, Elapsed Time: 17.129898071289062\n",
            "Episode 43/1000, Total Reward: -109.13043530722868, Elapsed Time: 0.5606904029846191\n",
            "Episode 44/1000, Total Reward: -103.94432099175341, Elapsed Time: 0.4120802879333496\n",
            "Episode 45/1000, Total Reward: -200.05439654092308, Elapsed Time: 21.312047719955444\n",
            "Episode 46/1000, Total Reward: -122.24677489742015, Elapsed Time: 0.8166711330413818\n",
            "Episode 47/1000, Total Reward: -116.18109466025157, Elapsed Time: 0.48551225662231445\n",
            "Episode 48/1000, Total Reward: -115.94060440661386, Elapsed Time: 0.36441659927368164\n",
            "Episode 49/1000, Total Reward: -121.36206679057355, Elapsed Time: 0.46845102310180664\n",
            "Episode 50/1000, Total Reward: -200.0334858391413, Elapsed Time: 18.76266860961914\n",
            "Episode 51/1000, Total Reward: -114.57713169476737, Elapsed Time: 0.3925762176513672\n",
            "Episode 52/1000, Total Reward: -117.94169687842515, Elapsed Time: 0.4771404266357422\n",
            "Episode 53/1000, Total Reward: -120.01757418078456, Elapsed Time: 0.46464014053344727\n",
            "Episode 54/1000, Total Reward: -102.03837848280817, Elapsed Time: 0.3152732849121094\n",
            "Episode 55/1000, Total Reward: -114.90489696878555, Elapsed Time: 0.45117712020874023\n",
            "Episode 56/1000, Total Reward: -104.61808361501967, Elapsed Time: 0.7079722881317139\n",
            "Episode 57/1000, Total Reward: -193.42190201496038, Elapsed Time: 7.013572692871094\n",
            "Episode 58/1000, Total Reward: -101.49388236330473, Elapsed Time: 0.5129265785217285\n",
            "Episode 59/1000, Total Reward: -122.82970010712293, Elapsed Time: 1.0236334800720215\n",
            "Episode 60/1000, Total Reward: -101.20305528501285, Elapsed Time: 0.5330862998962402\n",
            "Episode 61/1000, Total Reward: -104.14653958067521, Elapsed Time: 0.38143491744995117\n",
            "Episode 62/1000, Total Reward: -104.18187905994195, Elapsed Time: 0.4405698776245117\n",
            "Episode 63/1000, Total Reward: -200.0759566362041, Elapsed Time: 23.03694725036621\n",
            "Episode 64/1000, Total Reward: -100.6185873990907, Elapsed Time: 0.74029541015625\n",
            "Episode 65/1000, Total Reward: -116.86243520111529, Elapsed Time: 0.32129764556884766\n",
            "Episode 66/1000, Total Reward: -99.19784385984924, Elapsed Time: 0.3714334964752197\n",
            "Episode 67/1000, Total Reward: -107.92231412140603, Elapsed Time: 0.6981792449951172\n",
            "Episode 68/1000, Total Reward: -200.0166438028229, Elapsed Time: 20.149473190307617\n",
            "Episode 69/1000, Total Reward: -104.32515873116651, Elapsed Time: 0.33960962295532227\n",
            "Episode 70/1000, Total Reward: -100.47261041186079, Elapsed Time: 0.514531135559082\n",
            "Episode 71/1000, Total Reward: -103.18368303681054, Elapsed Time: 0.45762014389038086\n",
            "Episode 72/1000, Total Reward: -200.04422037919522, Elapsed Time: 20.819422245025635\n",
            "Episode 73/1000, Total Reward: -105.85989981471685, Elapsed Time: 0.346435546875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to be tested\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "# Definizione dell'ambiente\n",
        "ENV = \"BipedalWalker-v3\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPISODES = 100\n",
        "action_buckets = 11\n",
        "# Carica i parametri del modello salvato\n",
        "model_params = torch.load('dqn_model.pth')\n",
        "\n",
        "# Crea l'ambiente\n",
        "env = gym.make(ENV)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "observation = env.reset()[0]\n",
        "for i in range(EPISODES):\n",
        "    # Crea il modello e carica i parametri\n",
        "\n",
        "    model = QNetwork(state_dim, action_buckets ** action_dim).to(DEVICE)\n",
        "    model.load_state_dict(model_params)\n",
        "    model.eval()  # Imposta il modello in modalità valutazione\n",
        "\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Preprocessa l'osservazione se necessario\n",
        "        state = torch.tensor(observation).float().to(DEVICE)\n",
        "        q_values = model(state)\n",
        "\n",
        "        # Scegli l'azione con il massimo Q-value\n",
        "        discrete_action = q_values.argmax().item()\n",
        "        action = (discrete_action / (action_buckets - 1)) * (env.action_space.high - env.action_space.low) + env.action_space.low\n",
        "\n",
        "        # Esegui l'azione nell'ambiente\n",
        "        next_observation, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        observation = next_observation\n",
        "\n",
        "        # Visualizza l'ambiente (commenta questa linea se non vuoi visualizzare l'ambiente)\n",
        "        env.render()\n",
        "\n",
        "env.close()\n",
        "print(f\"Total Reward: {total_reward}\")\n"
      ],
      "metadata": {
        "id": "m45TsNgFoG07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "#test normalizer\n",
        "\n",
        "class Normalizer:\n",
        "    def __init__(self, num_inputs):\n",
        "        self.mean = np.zeros(num_inputs)\n",
        "        self.m2 = np.zeros(num_inputs)\n",
        "        self.count = 0\n",
        "\n",
        "    # Welford's online algorithm for update using unbiased variance\n",
        "    # more info: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm\n",
        "    def update(self, x):\n",
        "        self.count += 1\n",
        "        old_mean = self.mean.copy()\n",
        "        self.mean += (x - self.mean) / self.count\n",
        "        self.m2 += (x - old_mean) * (x - self.mean)\n",
        "\n",
        "    def normalize(self, x):\n",
        "        print(\"Input:\", x)\n",
        "        eps = 1e-10\n",
        "        mean = torch.tensor(self.mean).float().to(DEVICE)\n",
        "        if self.count > 1:\n",
        "            variance = self.m2 / (self.count - 1)\n",
        "        else:\n",
        "            variance = np.zeros_like(self.m2)\n",
        "        stdev = torch.tensor(np.sqrt(variance) + eps).float().to(DEVICE)\n",
        "        x = (x - mean) / (stdev)\n",
        "        print(\"Normalized:\", x)\n",
        "        print(\"Mean:\", mean)\n",
        "        print(\"Std Dev:\", stdev)\n",
        "        return x\n",
        "\n",
        "# Esempio di utilizzo\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Creare un'istanza di Normalizer con 24 dimensioni\n",
        "normalizer = Normalizer(num_inputs=24)\n",
        "\n",
        "# Generare alcuni campioni casuali e aggiornare il normalizzatore\n",
        "for _ in range(10):\n",
        "    data = np.random.randn(24)\n",
        "    normalizer.update(data)\n",
        "    data = torch.tensor(data).float().to(DEVICE)\n",
        "    normalized_data = normalizer.normalize(data)\n",
        "\n",
        "# Generare un nuovo campione casuale e normalizzarlo\n",
        "test_data = torch.tensor(np.random.randn(24)).float().to(DEVICE)\n",
        "normalized_data = normalizer.normalize(test_data)\n",
        "print(normalized_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAsB2PNILQJ_",
        "outputId": "8b819d6a-af31-4f13-910c-481cf661dd9e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: tensor([ 0.0617, -0.0794,  1.7334, -1.3270, -0.6230, -0.7431, -2.0793, -0.0973,\n",
            "         0.9154, -0.4157,  2.0541, -0.9760, -0.1498,  0.2996, -0.5457, -1.3555,\n",
            "         0.1059, -1.6788, -0.9164,  1.0210,  0.2366,  0.5093, -0.4097, -1.4385])\n",
            "Normalized: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Mean: tensor([ 0.0617, -0.0794,  1.7334, -1.3270, -0.6230, -0.7431, -2.0793, -0.0973,\n",
            "         0.9154, -0.4157,  2.0541, -0.9760, -0.1498,  0.2996, -0.5457, -1.3555,\n",
            "         0.1059, -1.6788, -0.9164,  1.0210,  0.2366,  0.5093, -0.4097, -1.4385])\n",
            "Std Dev: tensor([1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10,\n",
            "        1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10,\n",
            "        1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10,\n",
            "        1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10])\n",
            "Input: tensor([-1.2238,  0.2245, -0.6147, -0.6763,  0.8104, -0.4030, -1.5427, -2.1261,\n",
            "        -1.0199,  0.1357, -2.1121,  0.4138, -0.7490,  1.0266,  1.7039, -2.4091,\n",
            "        -0.3144,  2.0993,  1.4862,  1.3199,  0.1236,  1.1589, -0.3735, -1.9791])\n",
            "Normalized: tensor([-0.7071,  0.7071, -0.7071,  0.7071,  0.7071,  0.7071,  0.7071, -0.7071,\n",
            "        -0.7071,  0.7071, -0.7071,  0.7071, -0.7071,  0.7071,  0.7071, -0.7071,\n",
            "        -0.7071,  0.7071,  0.7071,  0.7071, -0.7071,  0.7071,  0.7071, -0.7071])\n",
            "Mean: tensor([-0.5810,  0.0725,  0.5593, -1.0017,  0.0937, -0.5731, -1.8110, -1.1117,\n",
            "        -0.0523, -0.1400, -0.0290, -0.2811, -0.4494,  0.6631,  0.5791, -1.8823,\n",
            "        -0.1043,  0.2103,  0.2849,  1.1705,  0.1801,  0.8341, -0.3916, -1.7088])\n",
            "Std Dev: tensor([0.9090, 0.2149, 1.6604, 0.4601, 1.0136, 0.2405, 0.3794, 1.4346, 1.3685,\n",
            "        0.3900, 2.9459, 0.9828, 0.4237, 0.5141, 1.5907, 0.7450, 0.2972, 2.6715,\n",
            "        1.6989, 0.2114, 0.0800, 0.4594, 0.0256, 0.3823])\n",
            "Input: tensor([ 0.1193, -1.2443,  0.8477, -0.1917,  0.3412,  0.6250,  0.3956, -1.2091,\n",
            "         1.0390,  0.1156, -0.9118, -0.5996, -0.8025,  0.4721,  0.3602,  0.2688,\n",
            "        -0.2081, -0.0970,  0.1349, -1.2502,  0.0141,  0.4211,  0.7983,  0.6700])\n",
            "Normalized: tensor([ 0.6148, -1.1323,  0.1622,  0.9479,  0.2258,  1.1213,  1.1299, -0.0639,\n",
            "         0.6301,  0.5448, -0.2744, -0.2954, -0.6496, -0.3353, -0.1289,  1.0630,\n",
            "        -0.3168, -0.1080, -0.0830, -1.1482, -0.9946, -0.6833,  1.1543,  1.1330])\n",
            "Mean: tensor([-0.3476, -0.3664,  0.6555, -0.7317,  0.1762, -0.1737, -1.0755, -1.1442,\n",
            "         0.3115, -0.0548, -0.3233, -0.3872, -0.5671,  0.5995,  0.5061, -1.1652,\n",
            "        -0.1389,  0.1079,  0.2349,  0.3636,  0.1248,  0.6964,  0.0050, -0.9159])\n",
            "Std Dev: tensor([0.7594, 0.7753, 1.1858, 0.5697, 0.7308, 0.7123, 1.3019, 1.0160, 1.1547,\n",
            "        0.3127, 2.1445, 0.7188, 0.3624, 0.3799, 1.1318, 1.3491, 0.2186, 1.8974,\n",
            "        1.2044, 1.4056, 0.1113, 0.4029, 0.6872, 1.3997])\n",
            "Input: tensor([ 0.3098,  2.2289,  0.5456, -1.6283,  2.8150,  0.2990,  0.0829,  0.0741,\n",
            "         0.8357, -0.1969,  1.3727,  1.5963,  2.0642, -0.8244,  0.2505, -0.0462,\n",
            "        -0.0944, -1.8749, -0.0857,  1.3501, -0.8547, -0.0926, -1.8326,  0.5588])\n",
            "Normalized: tensor([ 0.7026,  1.3481, -0.0850, -1.0409,  1.3667,  0.5648,  0.7177,  0.8878,\n",
            "         0.4018, -0.4022,  0.6538,  1.2909,  1.4634, -1.3752, -0.2055,  0.6793,\n",
            "         0.1853, -0.8085, -0.2413,  0.5923, -1.4748, -1.1520, -1.2802,  0.8132])\n",
            "Mean: tensor([-0.1833,  0.2824,  0.6280, -0.9558,  0.8359, -0.0555, -0.7859, -0.8396,\n",
            "         0.4425, -0.0903,  0.1007,  0.1086,  0.0907,  0.2435,  0.4422, -0.8855,\n",
            "        -0.1278, -0.3878,  0.1547,  0.6102, -0.1201,  0.4992, -0.4544, -0.5472])\n",
            "Std Dev: tensor([0.7018, 1.4439, 0.9698, 0.6460, 1.4481, 0.6278, 1.2105, 1.0292, 0.9785,\n",
            "        0.2651, 1.9455, 1.1524, 1.3485, 0.7766, 0.9329, 1.2355, 0.1798, 1.8392,\n",
            "        0.9964, 1.2491, 0.4981, 0.5137, 1.0766, 1.3601])\n",
            "Input: tensor([ 0.4592,  1.0586,  0.7521,  0.2122,  2.5483, -1.9443,  2.1447,  0.5830,\n",
            "        -0.7975, -0.3877, -0.9223, -0.2971,  2.0918, -0.5453, -0.1269, -0.8618,\n",
            "        -0.1329, -0.9118, -1.3471, -2.0115,  1.0518,  0.6795, -0.1429, -0.3322])\n",
            "Normalized: tensor([ 0.7646,  0.4785,  0.1180,  1.2208,  0.9323, -1.5042,  1.3969,  1.0393,\n",
            "        -0.9795, -0.8967, -0.4688, -0.3200,  1.0881, -0.8309, -0.5375,  0.0177,\n",
            "        -0.0264, -0.2603, -1.0987, -1.3147,  1.3812,  0.3190,  0.2643,  0.1456])\n",
            "Mean: tensor([-0.0548,  0.4377,  0.6528, -0.7222,  1.1784, -0.4333, -0.1998, -0.5551,\n",
            "         0.1945, -0.1498, -0.1039,  0.0275,  0.4910,  0.0857,  0.3284, -0.8808,\n",
            "        -0.1288, -0.4926, -0.1456,  0.0859,  0.1143,  0.5352, -0.3921, -0.5042])\n",
            "Std Dev: tensor([0.6722, 1.2977, 0.8417, 0.7654, 1.4694, 1.0045, 1.6783, 1.0951, 1.0128,\n",
            "        0.2653, 1.7459, 1.0144, 1.4713, 0.7594, 0.8471, 1.0700, 0.1558, 1.6100,\n",
            "        1.0935, 1.5953, 0.6788, 0.4521, 0.9427, 1.1818])\n",
            "Input: tensor([-0.3722,  0.9554,  0.2104,  0.6104, -1.3919,  0.5776,  0.1321, -0.7883,\n",
            "         0.8479, -0.0821,  1.5037,  0.8284,  0.5141,  0.1428, -0.3165,  0.3388,\n",
            "        -1.3644, -1.3679,  0.8496,  1.7281,  0.6984, -0.9931, -0.8166, -0.4930])\n",
            "Normalized: tensor([-0.4300,  0.3657, -0.4762,  1.2700, -1.2736,  0.8520,  0.1835, -0.1975,\n",
            "         0.5766,  0.2364,  0.7909,  0.6920,  0.0147,  0.0700, -0.6700,  0.9421,\n",
            "        -1.9676, -0.4916,  0.7831,  0.8680,  0.7463, -1.7129, -0.4110,  0.0088])\n",
            "Mean: tensor([-0.1077,  0.5239,  0.5791, -0.5001,  0.7500, -0.2648, -0.1444, -0.5939,\n",
            "         0.3034, -0.1385,  0.1640,  0.1610,  0.4948,  0.0952,  0.2209, -0.6775,\n",
            "        -0.3347, -0.6385,  0.0202,  0.3596,  0.2116,  0.2805, -0.4628, -0.5023])\n",
            "Std Dev: tensor([0.6151, 1.1798, 0.7742, 0.8744, 1.6818, 0.9887, 1.5072, 0.9841, 0.9443,\n",
            "        0.2389, 1.6939, 0.9644, 1.3160, 0.6796, 0.8021, 1.0788, 0.5233, 1.4837,\n",
            "        1.0591, 1.5765, 0.6523, 0.7435, 0.8608, 1.0570])\n",
            "Input: tensor([ 0.4933,  0.5714,  0.1147, -0.0041,  2.3797, -2.1573, -0.2508,  0.2712,\n",
            "        -0.4240, -0.0764,  0.5923, -1.4021, -0.0384,  2.0324,  0.7958, -0.0588,\n",
            "         0.2060,  2.7901,  0.4979, -1.3573,  0.6010,  1.0615,  0.6755,  0.2204])\n",
            "Normalized: tensor([ 0.8505,  0.0378, -0.5466,  0.5185,  0.8445, -1.4086, -0.0662,  0.7757,\n",
            "        -0.6891,  0.2429,  0.2361, -1.2636, -0.3752,  1.7301,  0.6451,  0.5239,\n",
            "         0.8920,  1.5678,  0.4163, -0.9322,  0.5441,  0.9044,  1.0891,  0.6177])\n",
            "Mean: tensor([-0.0218,  0.5307,  0.5128, -0.4293,  0.9828, -0.5352, -0.1596, -0.4703,\n",
            "         0.1995, -0.1296,  0.2252, -0.0623,  0.4186,  0.3720,  0.3030, -0.5891,\n",
            "        -0.2575, -0.1487,  0.0885,  0.1143,  0.2673,  0.3921, -0.3002, -0.3991])\n",
            "Std Dev: tensor([0.6057, 1.0771, 0.7282, 0.8200, 1.6542, 1.1517, 1.3765, 0.9560, 0.9048,\n",
            "        0.2193, 1.5547, 1.0602, 1.2182, 0.9597, 0.7638, 1.0122, 0.5196, 1.8745,\n",
            "        0.9835, 1.5787, 0.6133, 0.7402, 0.8959, 1.0029])\n",
            "Input: tensor([-1.1990, -0.0080,  3.1226, -2.4735, -1.7141,  2.1380,  0.4454, -2.0035,\n",
            "        -0.9796,  0.0155, -1.5584,  0.8461, -1.0180,  0.1767, -0.1680,  0.8448,\n",
            "         1.7499,  0.2032,  0.3365,  0.7539,  0.0350,  1.3989,  1.6866,  0.4063])\n",
            "Normalized: tensor([-1.4750, -0.4643,  1.9983, -1.7065, -1.3080,  1.6416,  0.4097, -1.2926,\n",
            "        -1.1026,  0.6062, -0.9931,  0.7696, -1.0163, -0.1917, -0.5673,  1.1776,\n",
            "         2.0486,  0.1770,  0.2372,  0.3784, -0.3542,  1.1409,  1.5994,  0.7256])\n",
            "Mean: tensor([-0.1690,  0.4634,  0.8390, -0.6848,  0.6457, -0.2010, -0.0840, -0.6620,\n",
            "         0.0521, -0.1115,  0.0023,  0.0512,  0.2391,  0.3476,  0.2442, -0.4099,\n",
            "        -0.0065, -0.1047,  0.1195,  0.1942,  0.2382,  0.5179, -0.0519, -0.2984])\n",
            "Std Dev: tensor([0.6983, 1.0153, 1.1428, 1.0482, 1.8041, 1.4248, 1.2922, 1.0379, 0.9357,\n",
            "        0.2094, 1.5715, 1.0328, 1.2369, 0.8912, 0.7265, 1.0654, 0.8574, 1.7399,\n",
            "        0.9148, 1.4790, 0.5738, 0.7722, 1.0869, 0.9711])\n",
            "Input: tensor([ 0.5470,  0.8358, -0.0050,  1.3866,  1.2620,  0.6048, -0.2120, -0.8449,\n",
            "         0.4695,  0.5907,  2.2002,  0.1308,  1.5396,  0.3388, -1.3565,  0.1665,\n",
            "        -1.3319,  0.9600,  0.1723,  1.6408,  0.3276, -0.5569, -0.2792,  1.0868])\n",
            "Normalized: tensor([ 0.9151,  0.3457, -0.6787,  1.5354,  0.3222,  0.5268, -0.0940, -0.1671,\n",
            "         0.4186,  2.0449,  1.1895,  0.0731,  0.9357, -0.0093, -1.6468,  0.5048,\n",
            "        -1.2866,  0.5681,  0.0548,  0.8776,  0.1478, -1.1850, -0.1982,  1.2083])\n",
            "Mean: tensor([-0.0894,  0.5048,  0.7452, -0.4546,  0.7142, -0.1115, -0.0982, -0.6823,\n",
            "         0.0985, -0.0335,  0.2465,  0.0601,  0.3836,  0.3466,  0.0663, -0.3458,\n",
            "        -0.1538,  0.0136,  0.1253,  0.3550,  0.2482,  0.3985, -0.0771, -0.1445])\n",
            "Std Dev: tensor([0.6955, 0.9578, 1.1054, 1.1992, 1.7000, 1.3596, 1.2095, 0.9728, 0.8862,\n",
            "        0.3052, 1.6425, 0.9665, 1.2356, 0.8336, 0.8640, 1.0150, 0.9157, 1.6658,\n",
            "        0.8559, 1.4651, 0.5375, 0.8063, 1.0195, 1.0190])\n",
            "Input: tensor([ 0.0286,  0.4459,  1.0762, -0.9970, -1.7522,  1.6034, -0.7991, -0.7488,\n",
            "         0.2462, -0.9199,  0.7884, -1.4403, -0.1231, -0.6969, -0.0995, -0.4075,\n",
            "        -2.3167,  0.2270, -1.8922,  1.7332,  0.7047,  0.4821,  0.2165, -0.1637])\n",
            "Normalized: tensor([ 0.1618, -0.0587,  0.2844, -0.4269, -1.2453,  1.1089, -0.5430, -0.0652,\n",
            "         0.1589, -1.9858,  0.3131, -1.3145, -0.3878, -1.1018, -0.1828, -0.0580,\n",
            "        -1.7674,  0.1222, -1.7652,  0.8564,  0.7797,  0.0989,  0.2736, -0.0179])\n",
            "Mean: tensor([-0.0776,  0.4989,  0.7783, -0.5089,  0.4675,  0.0600, -0.1683, -0.6890,\n",
            "         0.1133, -0.1221,  0.3007, -0.0900,  0.3329,  0.2422,  0.0497, -0.3520,\n",
            "        -0.3701,  0.0349, -0.0764,  0.4928,  0.2938,  0.4069, -0.0478, -0.1464])\n",
            "Std Dev: tensor([0.6567, 0.9032, 1.0474, 1.1435, 1.7825, 1.3918, 1.1617, 0.9174, 0.8369,\n",
            "        0.4017, 1.5580, 1.0273, 1.1759, 0.8524, 0.8163, 0.9571, 1.1014, 1.5719,\n",
            "        1.0287, 1.4484, 0.5269, 0.7607, 0.9657, 0.9608])\n",
            "Input: tensor([ 0.3552, -0.1989,  0.4591,  0.5933,  0.1104,  0.5338,  0.4214, -0.2770,\n",
            "        -0.9399, -0.5195, -0.4835, -0.1570,  0.0533, -0.5851, -0.2907, -0.6199,\n",
            "         0.6757, -0.5587,  0.5240,  3.3178,  0.2738,  0.2548, -1.3528, -1.1371])\n",
            "Normalized: tensor([ 0.6590, -0.7726, -0.3047,  0.9638, -0.2003,  0.3404,  0.5076,  0.4491,\n",
            "        -1.2585, -0.9892, -0.5033, -0.0653, -0.2378, -0.9706, -0.4170, -0.2799,\n",
            "         0.9495, -0.3776,  0.5837,  1.9504, -0.0380, -0.1999, -1.3514, -1.0311])\n",
            "Mean: tensor([-0.0776,  0.4989,  0.7783, -0.5089,  0.4675,  0.0600, -0.1683, -0.6890,\n",
            "         0.1133, -0.1221,  0.3007, -0.0900,  0.3329,  0.2422,  0.0497, -0.3520,\n",
            "        -0.3701,  0.0349, -0.0764,  0.4928,  0.2938,  0.4069, -0.0478, -0.1464])\n",
            "Std Dev: tensor([0.6567, 0.9032, 1.0474, 1.1435, 1.7825, 1.3918, 1.1617, 0.9174, 0.8369,\n",
            "        0.4017, 1.5580, 1.0273, 1.1759, 0.8524, 0.8163, 0.9571, 1.1014, 1.5719,\n",
            "        1.0287, 1.4484, 0.5269, 0.7607, 0.9657, 0.9608])\n",
            "tensor([ 0.6590, -0.7726, -0.3047,  0.9638, -0.2003,  0.3404,  0.5076,  0.4491,\n",
            "        -1.2585, -0.9892, -0.5033, -0.0653, -0.2378, -0.9706, -0.4170, -0.2799,\n",
            "         0.9495, -0.3776,  0.5837,  1.9504, -0.0380, -0.1999, -1.3514, -1.0311])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_N3I34eNgNrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Supponendo che discrete_action sia l'indice singolo che rappresenta l'azione discreta\n",
        "discrete_action = 123  # Esempio di indice\n",
        "\n",
        "# Calcoliamo gli indici per ogni dimensione delle azioni\n",
        "action_indices = np.unravel_index(discrete_action, [action_buckets] * env.action_space.shape[0])\n",
        "\n",
        "# Stampa degli indici per debug\n",
        "print(\"Indices for each action dimension:\", action_indices)\n",
        "\n",
        "# Se vuoi convertire questi indici in una tupla di azione discreta\n",
        "discrete_action_tuple = tuple(action_indices)\n",
        "\n",
        "# Stampa dell'azione discreta come tupla\n",
        "print(\"Discrete action as tuple:\", discrete_action_tuple)\n"
      ],
      "metadata": {
        "id": "kueZQeZOHKtZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}