{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVyD8VrwqH3O",
        "outputId": "b1dd9338-89e3-4fa5-8713-fcb6505524b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.2.1)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (0.3.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install dill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH3Y_cv0p3k4",
        "outputId": "14109e6e-abe4-403b-9fea-a9083795dde5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation Space: Box([-3.1415927 -5.        -5.        -5.        -3.1415927 -5.\n",
            " -3.1415927 -5.        -0.        -3.1415927 -5.        -3.1415927\n",
            " -5.        -0.        -1.        -1.        -1.        -1.\n",
            " -1.        -1.        -1.        -1.        -1.        -1.       ], [3.1415927 5.        5.        5.        3.1415927 5.        3.1415927\n",
            " 5.        5.        3.1415927 5.        3.1415927 5.        5.\n",
            " 1.        1.        1.        1.        1.        1.        1.\n",
            " 1.        1.        1.       ], (24,), float32)\n",
            "Action Space: Box(-1.0, 1.0, (4,), float32)\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import dill\n",
        "\n",
        "# Initialize the Bipedal Walker environment\n",
        "env = gym.make('BipedalWalker-v3')#, render_mode=\"human\")\n",
        "\n",
        "# Get the observation and action space\n",
        "obs = env.observation_space\n",
        "act = env.action_space\n",
        "\n",
        "print(f\"Observation Space: {obs}\")\n",
        "print(f\"Action Space: {act}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiRLelS9W5iX"
      },
      "outputs": [],
      "source": [
        "#hyperparameters\n",
        "OBS_BUCKETS = 11\n",
        "ACT_BUCKETS = 11\n",
        "EPISODES = 10000\n",
        "REWARD_THRESHOLD = -200\n",
        "GAMMA = 0.99\n",
        "ALPHA = 0.01\n",
        "EPSILON_INIT = 1.0\n",
        "EPSILON_DECAY = 0.9997\n",
        "EPSILON_MIN = 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZQoT-Zts0ze"
      },
      "outputs": [],
      "source": [
        "class QTableLearning:\n",
        "    def __init__(self, env, episodes=EPISODES, gamma=GAMMA, alpha=ALPHA, epsilon_init=EPSILON_INIT, epsilon_decay=EPSILON_DECAY, epsilon_min=EPSILON_MIN,\n",
        "                 obs_buckets=OBS_BUCKETS, act_buckets=ACT_BUCKETS, reward_threshold=REWARD_THRESHOLD, mode='uniform', render=False):\n",
        "        \"\"\"\n",
        "        Initialize the Q-learning agent.\n",
        "\n",
        "        Parameters:\n",
        "        env (gym.Env): The environment to be used.\n",
        "        obs_buckets (int): Number of discrete buckets per dimension in the observation space in uniform mode. Buckets per unity in range mode.\n",
        "        act_buckets (int): Number of discrete actions per dimension in the action space.\n",
        "        episodes (int): Number of episodes for training.\n",
        "        gamma (float): Discount factor.\n",
        "        alpha (float): Learning rate.\n",
        "        epsilon (float): Initial exploration rate.\n",
        "        epsilon_decay (float): Decay rate for epsilon.\n",
        "        epsilon_min (float): Minimum value for epsilon.\n",
        "        reward_threshold (float): Threshold for total reward to be considered a success.\n",
        "        mode (str): Discretization mode, 'uniform' or 'range'.\n",
        "        render (bool): Whether to render the environment.\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        # discretization parameters\n",
        "        self.obs_buckets = obs_buckets\n",
        "        self.act_buckets = act_buckets\n",
        "\n",
        "        # qtable initialization\n",
        "        self.qtable = defaultdict(lambda: np.zeros(tuple([self.act_buckets] * self.env.action_space.shape[0])))\n",
        "\n",
        "        # hyperparameters\n",
        "        self.episodes = episodes\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon_init\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.reward_threshold = reward_threshold\n",
        "\n",
        "        # select observation space discretization mode\n",
        "        # in range mode, obs_buckets is the number of buckets per unit\n",
        "        self.mode = mode\n",
        "\n",
        "        self.render = render\n",
        "        self.render_interval = 10\n",
        "\n",
        "    def discretizeState(self, state):\n",
        "        \"\"\"\n",
        "        Discretize the continuous observation state into discrete buckets based on the mode.\n",
        "\n",
        "        Parameters:\n",
        "        state (np.array): Continuous observation state.\n",
        "\n",
        "        Returns:\n",
        "        tuple: Discretized state.\n",
        "        \"\"\"\n",
        "        if self.mode == 'uniform':\n",
        "            discrete_state = np.round((state - self.env.observation_space.low) / (self.env.observation_space.high - self.env.observation_space.low) * (self.obs_buckets - 1)).astype(int)\n",
        "        elif self.mode == 'range':\n",
        "            discrete_state = []\n",
        "            for i, val in enumerate(state):\n",
        "                # compute the number of buckets for each dimension\n",
        "                bucket_size = (self.env.observation_space.high[i] - self.env.observation_space.low[i]) * self.obs_buckets\n",
        "                # make sure that there is at least one bucket for each dimension\n",
        "                bucket_size = max(1, int(bucket_size))\n",
        "                # discretize the dimension\n",
        "                discrete_val = round((val - self.env.observation_space.low[i]) / (self.env.observation_space.high[i] - self.env.observation_space.low[i]) * (bucket_size - 1))\n",
        "                discrete_state.append(discrete_val)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid mode '{self.mode}'. Supported modes are 'uniform' and 'range'.\")\n",
        "\n",
        "        return tuple(discrete_state)\n",
        "\n",
        "    def discretizeAction(self, action):\n",
        "        \"\"\"\n",
        "        Discretize the continuous action into discrete buckets.\n",
        "\n",
        "        Parameters:\n",
        "        action (np.array): Continuous action.\n",
        "\n",
        "        Returns:\n",
        "        tuple: Discretized action.\n",
        "        \"\"\"\n",
        "        discrete_action = np.round((action - self.env.action_space.low) / (self.env.action_space.high - self.env.action_space.low) * (self.act_buckets - 1)).astype(int)\n",
        "        return tuple(discrete_action)\n",
        "\n",
        "    def undiscretizeAction(self, action):\n",
        "        \"\"\"\n",
        "        Convert a discrete action back into a continuous action.\n",
        "\n",
        "        Parameters:\n",
        "        action (tuple): Discretized action.\n",
        "\n",
        "        Returns:\n",
        "        tuple: Continuous action.\n",
        "        \"\"\"\n",
        "        action = (np.array(action) / (self.act_buckets - 1)) * (self.env.action_space.high - self.env.action_space.low) + self.env.action_space.low\n",
        "        return tuple(action)\n",
        "\n",
        "    def epsilonGreedyStrategy(self, state):\n",
        "        \"\"\"\n",
        "        Choose an action using the epsilon-greedy strategy.\n",
        "\n",
        "        Parameters:\n",
        "        state (tuple): Current discretized state.\n",
        "\n",
        "        Returns:\n",
        "        np.array: Chosen action.\n",
        "        \"\"\"\n",
        "        if random.random() < self.epsilon:\n",
        "            # Exploration: choose a random action\n",
        "            action = np.random.randint(0, self.act_buckets, size=self.env.action_space.shape)\n",
        "        else:\n",
        "            # Exploitation: choose the action with the highest Q value\n",
        "            q_values = self.qtable[state]\n",
        "            flat_best_action_index = np.argmax(q_values)\n",
        "            action = np.array(np.unravel_index(flat_best_action_index, q_values.shape))\n",
        "        return action\n",
        "\n",
        "    def updateQTable(self, state, action, reward, next_state):\n",
        "        \"\"\"\n",
        "        Update the Q-table using the Q-learning update rule.\n",
        "\n",
        "        Parameters:\n",
        "        state (tuple): Current discretized state.\n",
        "        action (tuple): Discretized action taken.\n",
        "        reward (float): Reward received.\n",
        "        next_state (tuple): Next discretized state.\n",
        "        \"\"\"\n",
        "        q_sa = self.qtable[state][action]\n",
        "        max_next_value = np.max(self.qtable[next_state]) if next_state in self.qtable else 0\n",
        "        new_value = q_sa * (1 - self.alpha) + self.alpha * (reward + self.gamma * max_next_value)\n",
        "        self.qtable[state][action] = new_value\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"\n",
        "        Train the Q-learning agent over the specified number of episodes.\n",
        "        \"\"\"\n",
        "        rewards = []  # List to store total rewards for each episode\n",
        "        elapsed_times = []\n",
        "\n",
        "        for episode in range(1, self.episodes + 1):\n",
        "            total_reward = 0\n",
        "            steps_taken = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "            init = self.env.reset()[0]\n",
        "            state = self.discretizeState(init)\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "            while total_reward > self.reward_threshold:\n",
        "                action = self.epsilonGreedyStrategy(state)\n",
        "                continuous_action = self.undiscretizeAction(action)\n",
        "\n",
        "                next_state, reward, done, _, _ = self.env.step(continuous_action)\n",
        "\n",
        "                if self.render and steps_taken % self.render_interval == 0:\n",
        "                    self.env.render()\n",
        "\n",
        "                next_state = self.discretizeState(next_state)\n",
        "                action = tuple(action)\n",
        "                self.updateQTable(state, action, reward, next_state)\n",
        "                total_reward += reward\n",
        "                state = next_state\n",
        "                steps_taken += 1\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "            elapsed_times.append(elapsed_time)\n",
        "            rewards.append(total_reward)  # Store total reward for this episode\n",
        "            print(f\"Episode {episode}/{self.episodes}, Total Reward: {total_reward}, Elapsed Time: {elapsed_time}\")\n",
        "\n",
        "        self.env.close()\n",
        "        max_reward = max(rewards)  # Calculate the maximum reward\n",
        "        print(f\"Maximum Reward: {max_reward}\")\n",
        "        return self.qtable, rewards, elapsed_times\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-jdhkCLZ4wY"
      },
      "outputs": [],
      "source": [
        "window_size = 100\n",
        "\n",
        "# Function to calculate the moving average using np.mean\n",
        "def moving_average(data, window_size=window_size):\n",
        "    moving_averages = []\n",
        "    for i in range(len(data) - window_size + 1):\n",
        "        window = data[i:i + window_size]\n",
        "        window_average = np.mean(window)\n",
        "        moving_averages.append(window_average)\n",
        "    return moving_averages\n",
        "\n",
        "mode = 'range'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh14K6cMEE63",
        "outputId": "c52dde2b-a117-4f7d-ccf0-58e7d80fdea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1/10000, Total Reward: -116.32912175861486, Elapsed Time: 0.07074451446533203\n",
            "Episode 2/10000, Total Reward: -100.57599449975466, Elapsed Time: 0.08830046653747559\n",
            "Episode 3/10000, Total Reward: -200.15951100827013, Elapsed Time: 6.4485390186309814\n",
            "Episode 4/10000, Total Reward: -110.6305636057208, Elapsed Time: 0.079986572265625\n",
            "Episode 5/10000, Total Reward: -200.05213456081432, Elapsed Time: 6.003860712051392\n",
            "Episode 6/10000, Total Reward: -120.00448138479254, Elapsed Time: 0.2104027271270752\n",
            "Episode 7/10000, Total Reward: -110.96801700022294, Elapsed Time: 0.14368939399719238\n",
            "Episode 8/10000, Total Reward: -103.76239781713983, Elapsed Time: 0.11860227584838867\n",
            "Episode 9/10000, Total Reward: -110.82681858445852, Elapsed Time: 0.19338536262512207\n",
            "Episode 10/10000, Total Reward: -200.05425763810572, Elapsed Time: 7.557100534439087\n",
            "Episode 11/10000, Total Reward: -106.2679797379054, Elapsed Time: 0.098419189453125\n",
            "Episode 12/10000, Total Reward: -106.42156372790797, Elapsed Time: 0.10232186317443848\n",
            "Episode 13/10000, Total Reward: -125.89919246118528, Elapsed Time: 0.1572401523590088\n",
            "Episode 14/10000, Total Reward: -200.0353285945884, Elapsed Time: 2.9051146507263184\n",
            "Episode 15/10000, Total Reward: -200.18638917644282, Elapsed Time: 2.841047763824463\n",
            "Episode 16/10000, Total Reward: -109.73595569280299, Elapsed Time: 0.03881955146789551\n",
            "Episode 17/10000, Total Reward: -116.68060163352949, Elapsed Time: 0.06959366798400879\n",
            "Episode 18/10000, Total Reward: -200.081908179543, Elapsed Time: 4.07524299621582\n",
            "Episode 19/10000, Total Reward: -116.96021786307469, Elapsed Time: 0.10108637809753418\n",
            "Episode 20/10000, Total Reward: -200.0368890550855, Elapsed Time: 2.767179489135742\n",
            "Episode 21/10000, Total Reward: -114.51587149142325, Elapsed Time: 0.055051565170288086\n",
            "Episode 22/10000, Total Reward: -108.71961694947319, Elapsed Time: 0.04436492919921875\n",
            "Episode 23/10000, Total Reward: -111.59827328943288, Elapsed Time: 0.04789018630981445\n",
            "Episode 24/10000, Total Reward: -200.00636924959235, Elapsed Time: 2.850174903869629\n",
            "Episode 25/10000, Total Reward: -109.76368306821733, Elapsed Time: 0.03896164894104004\n",
            "Episode 26/10000, Total Reward: -200.14217447188594, Elapsed Time: 3.34775447845459\n",
            "Episode 27/10000, Total Reward: -102.00168051684747, Elapsed Time: 0.04321551322937012\n",
            "Episode 28/10000, Total Reward: -103.24845696899071, Elapsed Time: 0.052073001861572266\n",
            "Episode 29/10000, Total Reward: -200.09269807531712, Elapsed Time: 3.932722568511963\n",
            "Episode 30/10000, Total Reward: -200.00828994194399, Elapsed Time: 2.583545446395874\n",
            "Episode 31/10000, Total Reward: -121.59821868356242, Elapsed Time: 0.10458588600158691\n",
            "Episode 32/10000, Total Reward: -101.57455707138963, Elapsed Time: 0.046904563903808594\n",
            "Episode 33/10000, Total Reward: -200.0141193234066, Elapsed Time: 2.6392877101898193\n",
            "Episode 34/10000, Total Reward: -200.07428858558595, Elapsed Time: 2.743776798248291\n",
            "Episode 35/10000, Total Reward: -98.74989566692983, Elapsed Time: 0.058031558990478516\n",
            "Episode 36/10000, Total Reward: -200.04406857396043, Elapsed Time: 2.9156088829040527\n",
            "Episode 37/10000, Total Reward: -106.17901605182725, Elapsed Time: 0.09908771514892578\n",
            "Episode 38/10000, Total Reward: -113.8692591718141, Elapsed Time: 0.06624960899353027\n",
            "Episode 39/10000, Total Reward: -100.08741504253472, Elapsed Time: 0.11790657043457031\n",
            "Episode 40/10000, Total Reward: -121.42334768812196, Elapsed Time: 0.13609743118286133\n",
            "Episode 41/10000, Total Reward: -200.1297026561597, Elapsed Time: 3.209414005279541\n",
            "Episode 42/10000, Total Reward: -200.08791864248448, Elapsed Time: 2.732536554336548\n",
            "Episode 43/10000, Total Reward: -101.09438254135289, Elapsed Time: 0.04742264747619629\n",
            "Episode 44/10000, Total Reward: -100.53173960760907, Elapsed Time: 0.05847311019897461\n",
            "Episode 45/10000, Total Reward: -105.71176754788087, Elapsed Time: 0.07387590408325195\n",
            "Episode 46/10000, Total Reward: -100.88761451713505, Elapsed Time: 0.05273914337158203\n",
            "Episode 47/10000, Total Reward: -109.73964125639026, Elapsed Time: 0.038865089416503906\n",
            "Episode 48/10000, Total Reward: -112.20652558317607, Elapsed Time: 0.07679224014282227\n",
            "Episode 49/10000, Total Reward: -99.47853953512832, Elapsed Time: 0.05755281448364258\n",
            "Episode 50/10000, Total Reward: -200.05671980282776, Elapsed Time: 2.399238348007202\n",
            "Episode 51/10000, Total Reward: -102.00240118548908, Elapsed Time: 0.07611298561096191\n",
            "Episode 52/10000, Total Reward: -111.57390758295793, Elapsed Time: 0.05305790901184082\n",
            "Episode 53/10000, Total Reward: -102.34290878704327, Elapsed Time: 0.07597184181213379\n",
            "Episode 54/10000, Total Reward: -106.58895568269342, Elapsed Time: 0.04588913917541504\n",
            "Episode 55/10000, Total Reward: -102.78814951057794, Elapsed Time: 0.09777188301086426\n",
            "Episode 56/10000, Total Reward: -119.36048120103776, Elapsed Time: 0.06615686416625977\n",
            "Episode 57/10000, Total Reward: -110.31609073396511, Elapsed Time: 0.04352521896362305\n",
            "Episode 58/10000, Total Reward: -107.08327582123988, Elapsed Time: 0.05824637413024902\n",
            "Episode 59/10000, Total Reward: -103.80201732984384, Elapsed Time: 0.05127286911010742\n",
            "Episode 60/10000, Total Reward: -200.09776094572527, Elapsed Time: 3.178534746170044\n",
            "Episode 61/10000, Total Reward: -100.08426374962467, Elapsed Time: 0.15526175498962402\n",
            "Episode 62/10000, Total Reward: -120.55881672672356, Elapsed Time: 0.1276230812072754\n",
            "Episode 63/10000, Total Reward: -137.1393163539828, Elapsed Time: 0.24347376823425293\n",
            "Episode 64/10000, Total Reward: -100.93175344847279, Elapsed Time: 0.08206534385681152\n",
            "Episode 65/10000, Total Reward: -200.09965335338578, Elapsed Time: 3.4381918907165527\n",
            "Episode 66/10000, Total Reward: -116.5471516601765, Elapsed Time: 0.0709996223449707\n",
            "Episode 67/10000, Total Reward: -126.1564693146877, Elapsed Time: 0.07744097709655762\n",
            "Episode 68/10000, Total Reward: -101.6495278144865, Elapsed Time: 0.087677001953125\n",
            "Episode 69/10000, Total Reward: -118.07322057082715, Elapsed Time: 0.0460357666015625\n",
            "Episode 70/10000, Total Reward: -142.3646765885847, Elapsed Time: 0.32882261276245117\n",
            "Episode 71/10000, Total Reward: -200.0150411736929, Elapsed Time: 4.912344217300415\n",
            "Episode 72/10000, Total Reward: -113.12596392295373, Elapsed Time: 0.10386228561401367\n",
            "Episode 73/10000, Total Reward: -107.83674846374318, Elapsed Time: 0.17176008224487305\n",
            "Episode 74/10000, Total Reward: -114.70658855481135, Elapsed Time: 0.23545193672180176\n",
            "Episode 75/10000, Total Reward: -114.8558869518362, Elapsed Time: 0.2275853157043457\n",
            "Episode 76/10000, Total Reward: -200.10654230959832, Elapsed Time: 5.168096542358398\n",
            "Episode 77/10000, Total Reward: -101.3392464487652, Elapsed Time: 0.18131089210510254\n",
            "Episode 78/10000, Total Reward: -106.65239353847653, Elapsed Time: 0.21777129173278809\n",
            "Episode 79/10000, Total Reward: -117.65619794182578, Elapsed Time: 0.08749246597290039\n",
            "Episode 80/10000, Total Reward: -108.4905429969995, Elapsed Time: 0.1364912986755371\n",
            "Episode 81/10000, Total Reward: -101.60836396099317, Elapsed Time: 0.09474730491638184\n",
            "Episode 82/10000, Total Reward: -102.2311684784174, Elapsed Time: 0.09716629981994629\n",
            "Episode 83/10000, Total Reward: -113.59116048522567, Elapsed Time: 0.14239954948425293\n",
            "Episode 84/10000, Total Reward: -106.13201945596647, Elapsed Time: 0.10243749618530273\n",
            "Episode 85/10000, Total Reward: -108.25998560298905, Elapsed Time: 0.07475948333740234\n",
            "Episode 86/10000, Total Reward: -98.30401204827949, Elapsed Time: 0.1578524112701416\n",
            "Episode 87/10000, Total Reward: -200.32423664084052, Elapsed Time: 5.145781993865967\n",
            "Episode 88/10000, Total Reward: -121.29140382494367, Elapsed Time: 0.07070493698120117\n",
            "Episode 89/10000, Total Reward: -200.12574332539006, Elapsed Time: 2.777012825012207\n",
            "Episode 90/10000, Total Reward: -200.00589870204053, Elapsed Time: 3.7200052738189697\n",
            "Episode 91/10000, Total Reward: -122.16370522166591, Elapsed Time: 0.28493690490722656\n",
            "Episode 92/10000, Total Reward: -217.93697368396107, Elapsed Time: 1.9940011501312256\n",
            "Episode 93/10000, Total Reward: -111.32106622335911, Elapsed Time: 0.09457802772521973\n",
            "Episode 94/10000, Total Reward: -200.067905657844, Elapsed Time: 3.1956398487091064\n",
            "Episode 95/10000, Total Reward: -119.40948074954176, Elapsed Time: 0.05472898483276367\n",
            "Episode 96/10000, Total Reward: -121.46544339659077, Elapsed Time: 0.06495904922485352\n",
            "Episode 97/10000, Total Reward: -102.27839438366219, Elapsed Time: 0.05806255340576172\n",
            "Episode 98/10000, Total Reward: -236.12961655389125, Elapsed Time: 1.6328747272491455\n",
            "Episode 99/10000, Total Reward: -200.09229980337847, Elapsed Time: 2.3966832160949707\n",
            "Episode 100/10000, Total Reward: -99.17335941574859, Elapsed Time: 0.04246997833251953\n",
            "Episode 101/10000, Total Reward: -105.16472248409626, Elapsed Time: 0.04881119728088379\n",
            "Episode 102/10000, Total Reward: -114.10106693556185, Elapsed Time: 0.04705381393432617\n",
            "Episode 103/10000, Total Reward: -108.68712657861872, Elapsed Time: 0.07455039024353027\n",
            "Episode 104/10000, Total Reward: -100.79378904556098, Elapsed Time: 0.056524038314819336\n",
            "Episode 105/10000, Total Reward: -101.80530098049927, Elapsed Time: 0.06525039672851562\n",
            "Episode 106/10000, Total Reward: -200.1825962407216, Elapsed Time: 3.8343777656555176\n",
            "Episode 107/10000, Total Reward: -116.02666246780865, Elapsed Time: 0.04107213020324707\n",
            "Episode 108/10000, Total Reward: -99.82829614976123, Elapsed Time: 0.05450248718261719\n",
            "Episode 109/10000, Total Reward: -106.70909045635076, Elapsed Time: 0.0453188419342041\n",
            "Episode 110/10000, Total Reward: -117.63494683520595, Elapsed Time: 0.06206250190734863\n",
            "Episode 111/10000, Total Reward: -200.00382694614214, Elapsed Time: 4.084101438522339\n",
            "Episode 112/10000, Total Reward: -143.53942400383474, Elapsed Time: 0.27710485458374023\n",
            "Episode 113/10000, Total Reward: -121.03981721397341, Elapsed Time: 0.06177878379821777\n",
            "Episode 114/10000, Total Reward: -113.71879403569822, Elapsed Time: 0.041280269622802734\n",
            "Episode 115/10000, Total Reward: -200.10808663183852, Elapsed Time: 3.0559685230255127\n",
            "Episode 116/10000, Total Reward: -110.84770284475485, Elapsed Time: 0.0442051887512207\n",
            "Episode 117/10000, Total Reward: -99.63021868044498, Elapsed Time: 0.07492232322692871\n",
            "Episode 118/10000, Total Reward: -200.00935777710978, Elapsed Time: 3.3842933177948\n",
            "Episode 119/10000, Total Reward: -117.21983031147235, Elapsed Time: 0.08162188529968262\n",
            "Episode 120/10000, Total Reward: -117.04473105599644, Elapsed Time: 0.10323238372802734\n",
            "Episode 121/10000, Total Reward: -114.27608025895444, Elapsed Time: 0.09080958366394043\n",
            "Episode 122/10000, Total Reward: -102.92671555195898, Elapsed Time: 0.09731125831604004\n",
            "Episode 123/10000, Total Reward: -119.19566513922487, Elapsed Time: 0.08474230766296387\n",
            "Episode 124/10000, Total Reward: -103.76145570728418, Elapsed Time: 0.16951990127563477\n",
            "Episode 125/10000, Total Reward: -105.67137258983603, Elapsed Time: 0.08576703071594238\n",
            "Episode 126/10000, Total Reward: -113.22939295885327, Elapsed Time: 0.07581567764282227\n",
            "Episode 127/10000, Total Reward: -200.02655584352672, Elapsed Time: 3.0830881595611572\n",
            "Episode 128/10000, Total Reward: -109.9818378153062, Elapsed Time: 0.06500387191772461\n",
            "Episode 129/10000, Total Reward: -134.5174110815306, Elapsed Time: 0.29734349250793457\n",
            "Episode 130/10000, Total Reward: -119.04165600084947, Elapsed Time: 0.13691949844360352\n",
            "Episode 131/10000, Total Reward: -200.09811982324783, Elapsed Time: 4.087596654891968\n",
            "Episode 132/10000, Total Reward: -117.7244971833013, Elapsed Time: 0.057381391525268555\n",
            "Episode 133/10000, Total Reward: -118.46041309404708, Elapsed Time: 0.04948139190673828\n",
            "Episode 134/10000, Total Reward: -99.4429719310876, Elapsed Time: 0.05983853340148926\n",
            "Episode 135/10000, Total Reward: -101.23765779261018, Elapsed Time: 0.12871408462524414\n",
            "Episode 136/10000, Total Reward: -200.1199085559447, Elapsed Time: 2.679841995239258\n",
            "Episode 137/10000, Total Reward: -104.1290155250111, Elapsed Time: 0.07841944694519043\n",
            "Episode 138/10000, Total Reward: -103.19327633990657, Elapsed Time: 0.13149094581604004\n",
            "Episode 139/10000, Total Reward: -100.42363744132929, Elapsed Time: 0.19014883041381836\n",
            "Episode 140/10000, Total Reward: -200.15820243654585, Elapsed Time: 3.888570785522461\n",
            "Episode 141/10000, Total Reward: -101.40056239408615, Elapsed Time: 0.06215834617614746\n",
            "Episode 142/10000, Total Reward: -114.2977920693893, Elapsed Time: 0.05484819412231445\n",
            "Episode 143/10000, Total Reward: -111.73793704992372, Elapsed Time: 0.05492711067199707\n",
            "Episode 144/10000, Total Reward: -118.72332028916044, Elapsed Time: 0.08454465866088867\n",
            "Episode 145/10000, Total Reward: -100.13095855020856, Elapsed Time: 0.05678677558898926\n",
            "Episode 146/10000, Total Reward: -117.5469139040634, Elapsed Time: 0.09110760688781738\n",
            "Episode 147/10000, Total Reward: -115.257692695286, Elapsed Time: 0.08026599884033203\n",
            "Episode 148/10000, Total Reward: -102.51699171312414, Elapsed Time: 0.05329155921936035\n",
            "Episode 149/10000, Total Reward: -98.48316973750666, Elapsed Time: 0.08017635345458984\n",
            "Episode 150/10000, Total Reward: -200.00192146402725, Elapsed Time: 2.6263372898101807\n",
            "Episode 151/10000, Total Reward: -101.95927689028258, Elapsed Time: 0.05499887466430664\n",
            "Episode 152/10000, Total Reward: -107.51867253899525, Elapsed Time: 0.03898119926452637\n",
            "Episode 153/10000, Total Reward: -103.74188504266093, Elapsed Time: 0.07200145721435547\n",
            "Episode 154/10000, Total Reward: -107.66296711154095, Elapsed Time: 0.13995146751403809\n",
            "Episode 155/10000, Total Reward: -103.69172027349298, Elapsed Time: 0.07366657257080078\n",
            "Episode 156/10000, Total Reward: -102.52710151035276, Elapsed Time: 0.05183911323547363\n",
            "Episode 157/10000, Total Reward: -104.89672209153784, Elapsed Time: 0.06917166709899902\n",
            "Episode 158/10000, Total Reward: -200.0949043133016, Elapsed Time: 2.5433266162872314\n",
            "Episode 159/10000, Total Reward: -109.15389550576272, Elapsed Time: 0.050493717193603516\n",
            "Episode 160/10000, Total Reward: -102.20156815106546, Elapsed Time: 0.04121661186218262\n",
            "Episode 161/10000, Total Reward: -200.25528557683833, Elapsed Time: 2.637110710144043\n",
            "Episode 162/10000, Total Reward: -116.73180458212296, Elapsed Time: 0.1023402214050293\n",
            "Episode 163/10000, Total Reward: -101.17401797555722, Elapsed Time: 0.11789107322692871\n",
            "Episode 164/10000, Total Reward: -248.50041985921078, Elapsed Time: 2.559236526489258\n",
            "Episode 165/10000, Total Reward: -99.92062242421731, Elapsed Time: 0.04350471496582031\n",
            "Episode 166/10000, Total Reward: -228.7162530795888, Elapsed Time: 1.3481390476226807\n",
            "Episode 167/10000, Total Reward: -200.0283511620806, Elapsed Time: 2.7369072437286377\n",
            "Episode 168/10000, Total Reward: -137.20447415816335, Elapsed Time: 0.3029799461364746\n",
            "Episode 169/10000, Total Reward: -102.61480524610107, Elapsed Time: 0.06479978561401367\n",
            "Episode 170/10000, Total Reward: -200.02024540297109, Elapsed Time: 2.6485836505889893\n",
            "Episode 171/10000, Total Reward: -98.20348654556027, Elapsed Time: 0.07689213752746582\n",
            "Episode 172/10000, Total Reward: -99.40119731855789, Elapsed Time: 0.05643582344055176\n",
            "Episode 173/10000, Total Reward: -118.467074208969, Elapsed Time: 0.09392333030700684\n",
            "Episode 174/10000, Total Reward: -97.72297372158667, Elapsed Time: 0.07567477226257324\n",
            "Episode 175/10000, Total Reward: -125.6178177624767, Elapsed Time: 0.1274254322052002\n",
            "Episode 176/10000, Total Reward: -101.90005175887222, Elapsed Time: 0.06790542602539062\n",
            "Episode 177/10000, Total Reward: -178.94599491506244, Elapsed Time: 0.9161503314971924\n",
            "Episode 178/10000, Total Reward: -99.43549075545532, Elapsed Time: 0.1025686264038086\n",
            "Episode 179/10000, Total Reward: -99.9277337471243, Elapsed Time: 0.08305001258850098\n",
            "Episode 180/10000, Total Reward: -98.95609885672144, Elapsed Time: 0.09100222587585449\n",
            "Episode 181/10000, Total Reward: -197.43344986802907, Elapsed Time: 1.1021428108215332\n",
            "Episode 182/10000, Total Reward: -106.1745222501078, Elapsed Time: 0.10399174690246582\n",
            "Episode 183/10000, Total Reward: -200.20428615440136, Elapsed Time: 3.60119366645813\n",
            "Episode 184/10000, Total Reward: -118.34571284675052, Elapsed Time: 0.06720733642578125\n",
            "Episode 185/10000, Total Reward: -119.71579610722102, Elapsed Time: 0.06637263298034668\n",
            "Episode 186/10000, Total Reward: -119.2510194802535, Elapsed Time: 0.07886624336242676\n",
            "Episode 187/10000, Total Reward: -101.02902876273207, Elapsed Time: 0.07064127922058105\n",
            "Episode 188/10000, Total Reward: -200.03074890099347, Elapsed Time: 2.5623793601989746\n",
            "Episode 189/10000, Total Reward: -200.07135928859725, Elapsed Time: 2.439495801925659\n",
            "Episode 190/10000, Total Reward: -100.83445741327976, Elapsed Time: 0.06056714057922363\n",
            "Episode 191/10000, Total Reward: -107.0968384999567, Elapsed Time: 0.0861966609954834\n",
            "Episode 192/10000, Total Reward: -121.91166011186739, Elapsed Time: 0.09051990509033203\n",
            "Episode 193/10000, Total Reward: -120.25085315964868, Elapsed Time: 0.10701942443847656\n",
            "Episode 194/10000, Total Reward: -102.05974575697071, Elapsed Time: 0.08454060554504395\n",
            "Episode 195/10000, Total Reward: -127.94782798024539, Elapsed Time: 0.10843467712402344\n",
            "Episode 196/10000, Total Reward: -200.0373534132341, Elapsed Time: 2.615011215209961\n",
            "Episode 197/10000, Total Reward: -103.33577766715909, Elapsed Time: 0.04079008102416992\n",
            "Episode 198/10000, Total Reward: -98.19076293219042, Elapsed Time: 0.07092475891113281\n",
            "Episode 199/10000, Total Reward: -120.94638531843981, Elapsed Time: 0.0856013298034668\n",
            "Episode 200/10000, Total Reward: -116.21522711321055, Elapsed Time: 0.07608509063720703\n",
            "Episode 201/10000, Total Reward: -99.1554691410225, Elapsed Time: 0.06315183639526367\n",
            "Episode 202/10000, Total Reward: -162.18164548498373, Elapsed Time: 0.49242639541625977\n",
            "Episode 203/10000, Total Reward: -108.40153193652853, Elapsed Time: 0.1414017677307129\n",
            "Episode 204/10000, Total Reward: -200.08449686649382, Elapsed Time: 3.6760408878326416\n",
            "Episode 205/10000, Total Reward: -103.72708143187003, Elapsed Time: 0.05683612823486328\n",
            "Episode 206/10000, Total Reward: -103.23378769587961, Elapsed Time: 0.04446911811828613\n",
            "Episode 207/10000, Total Reward: -102.1236511153118, Elapsed Time: 0.056534767150878906\n",
            "Episode 208/10000, Total Reward: -200.07961145886347, Elapsed Time: 2.43904972076416\n",
            "Episode 209/10000, Total Reward: -100.91168687732133, Elapsed Time: 0.05751991271972656\n",
            "Episode 210/10000, Total Reward: -107.61596293468749, Elapsed Time: 0.06537079811096191\n",
            "Episode 211/10000, Total Reward: -111.06586266716458, Elapsed Time: 0.03667092323303223\n",
            "Episode 212/10000, Total Reward: -109.21895508904507, Elapsed Time: 0.0574188232421875\n",
            "Episode 213/10000, Total Reward: -103.30934188999422, Elapsed Time: 0.06892633438110352\n",
            "Episode 214/10000, Total Reward: -104.30773669316433, Elapsed Time: 0.07673978805541992\n",
            "Episode 215/10000, Total Reward: -108.27773194768243, Elapsed Time: 0.03744983673095703\n",
            "Episode 216/10000, Total Reward: -103.40706786332701, Elapsed Time: 0.07092571258544922\n",
            "Episode 217/10000, Total Reward: -107.09538896454362, Elapsed Time: 0.08817815780639648\n",
            "Episode 218/10000, Total Reward: -98.67498799557785, Elapsed Time: 0.06815814971923828\n",
            "Episode 219/10000, Total Reward: -106.5814274183395, Elapsed Time: 0.06713247299194336\n",
            "Episode 220/10000, Total Reward: -96.79465774771236, Elapsed Time: 0.061615705490112305\n",
            "Episode 221/10000, Total Reward: -108.93436164448174, Elapsed Time: 0.04783344268798828\n",
            "Episode 222/10000, Total Reward: -118.56474773871352, Elapsed Time: 0.10459661483764648\n",
            "Episode 223/10000, Total Reward: -100.99060346120707, Elapsed Time: 0.21921634674072266\n",
            "Episode 224/10000, Total Reward: -100.15763913988049, Elapsed Time: 0.30177998542785645\n",
            "Episode 225/10000, Total Reward: -100.42248379687679, Elapsed Time: 0.08465385437011719\n",
            "Episode 226/10000, Total Reward: -118.54464486105343, Elapsed Time: 0.0778050422668457\n",
            "Episode 227/10000, Total Reward: -99.8808054118786, Elapsed Time: 0.09363985061645508\n",
            "Episode 228/10000, Total Reward: -102.1929835519726, Elapsed Time: 0.04859161376953125\n",
            "Episode 229/10000, Total Reward: -99.53780318676978, Elapsed Time: 0.04406428337097168\n",
            "Episode 230/10000, Total Reward: -105.81723013500844, Elapsed Time: 0.0603632926940918\n",
            "Episode 231/10000, Total Reward: -200.09251288314465, Elapsed Time: 2.634366273880005\n",
            "Episode 232/10000, Total Reward: -200.000115247065, Elapsed Time: 2.311825752258301\n",
            "Episode 233/10000, Total Reward: -128.11907498537738, Elapsed Time: 0.1342148780822754\n",
            "Episode 234/10000, Total Reward: -114.46732669894149, Elapsed Time: 0.05250358581542969\n",
            "Episode 235/10000, Total Reward: -99.18454271941707, Elapsed Time: 0.08453512191772461\n",
            "Episode 236/10000, Total Reward: -104.43609654386653, Elapsed Time: 0.09238600730895996\n",
            "Episode 237/10000, Total Reward: -102.37207046988706, Elapsed Time: 0.12722420692443848\n",
            "Episode 238/10000, Total Reward: -108.18892641539598, Elapsed Time: 0.1312255859375\n",
            "Episode 239/10000, Total Reward: -104.89331830948181, Elapsed Time: 0.13825225830078125\n",
            "Episode 240/10000, Total Reward: -100.74702762143934, Elapsed Time: 0.08947563171386719\n",
            "Episode 241/10000, Total Reward: -120.26568705148026, Elapsed Time: 0.16555213928222656\n",
            "Episode 242/10000, Total Reward: -200.0399832167002, Elapsed Time: 3.2404448986053467\n",
            "Episode 243/10000, Total Reward: -104.63233495867377, Elapsed Time: 0.05002284049987793\n",
            "Episode 244/10000, Total Reward: -100.67104004335528, Elapsed Time: 0.049422502517700195\n",
            "Episode 245/10000, Total Reward: -103.72641674579394, Elapsed Time: 0.0687553882598877\n",
            "Episode 246/10000, Total Reward: -102.07669454638722, Elapsed Time: 0.05534529685974121\n",
            "Episode 247/10000, Total Reward: -241.86905080616887, Elapsed Time: 1.7663211822509766\n",
            "Episode 248/10000, Total Reward: -200.13798347071378, Elapsed Time: 2.7308995723724365\n",
            "Episode 249/10000, Total Reward: -99.62277483079558, Elapsed Time: 0.04875779151916504\n",
            "Episode 250/10000, Total Reward: -128.54367960498394, Elapsed Time: 0.10975027084350586\n",
            "Episode 251/10000, Total Reward: -100.69890533689274, Elapsed Time: 0.05901169776916504\n",
            "Episode 252/10000, Total Reward: -102.06984809331472, Elapsed Time: 0.06710672378540039\n",
            "Episode 253/10000, Total Reward: -102.75180471661365, Elapsed Time: 0.1501152515411377\n",
            "Episode 254/10000, Total Reward: -100.45988681150885, Elapsed Time: 0.0761101245880127\n",
            "Episode 255/10000, Total Reward: -104.34958122797372, Elapsed Time: 0.05855298042297363\n",
            "Episode 256/10000, Total Reward: -107.81648618017311, Elapsed Time: 0.036644697189331055\n",
            "Episode 257/10000, Total Reward: -116.01704140881847, Elapsed Time: 0.051564693450927734\n",
            "Episode 258/10000, Total Reward: -118.66914630362913, Elapsed Time: 0.05502581596374512\n",
            "Episode 259/10000, Total Reward: -200.0190933994055, Elapsed Time: 2.666433334350586\n",
            "Episode 260/10000, Total Reward: -103.80113892115838, Elapsed Time: 0.06196737289428711\n",
            "Episode 261/10000, Total Reward: -104.64396986641772, Elapsed Time: 0.05218982696533203\n",
            "Episode 262/10000, Total Reward: -110.4210998336458, Elapsed Time: 0.06129121780395508\n",
            "Episode 263/10000, Total Reward: -105.31623152973653, Elapsed Time: 0.07596135139465332\n",
            "Episode 264/10000, Total Reward: -100.38690785348155, Elapsed Time: 0.0908355712890625\n",
            "Episode 265/10000, Total Reward: -104.4826822009536, Elapsed Time: 0.08800959587097168\n",
            "Episode 266/10000, Total Reward: -101.19891503036258, Elapsed Time: 0.05889582633972168\n",
            "Episode 267/10000, Total Reward: -106.69704888569973, Elapsed Time: 0.08560919761657715\n",
            "Episode 268/10000, Total Reward: -128.03940661204868, Elapsed Time: 0.10491228103637695\n",
            "Episode 269/10000, Total Reward: -99.25170198965371, Elapsed Time: 0.07444930076599121\n",
            "Episode 270/10000, Total Reward: -121.52573017552717, Elapsed Time: 0.06689167022705078\n",
            "Episode 271/10000, Total Reward: -105.97110201443769, Elapsed Time: 0.06443500518798828\n",
            "Episode 272/10000, Total Reward: -120.41180741487008, Elapsed Time: 0.08609628677368164\n",
            "Episode 273/10000, Total Reward: -109.96896510949222, Elapsed Time: 0.06454277038574219\n",
            "Episode 274/10000, Total Reward: -103.64645015672532, Elapsed Time: 0.11677384376525879\n",
            "Episode 275/10000, Total Reward: -102.20187204543402, Elapsed Time: 0.09283447265625\n",
            "Episode 276/10000, Total Reward: -125.15165274072415, Elapsed Time: 0.10843181610107422\n",
            "Episode 277/10000, Total Reward: -127.66840770644606, Elapsed Time: 0.17669415473937988\n",
            "Episode 278/10000, Total Reward: -99.80581643942271, Elapsed Time: 0.0704958438873291\n",
            "Episode 279/10000, Total Reward: -116.18060016346263, Elapsed Time: 0.08863401412963867\n",
            "Episode 280/10000, Total Reward: -120.73535039729067, Elapsed Time: 0.11197781562805176\n",
            "Episode 281/10000, Total Reward: -105.10087813429473, Elapsed Time: 0.12230658531188965\n",
            "Episode 282/10000, Total Reward: -100.91515178728675, Elapsed Time: 0.13091206550598145\n",
            "Episode 283/10000, Total Reward: -162.56993597720395, Elapsed Time: 0.8177871704101562\n",
            "Episode 284/10000, Total Reward: -99.87580340205977, Elapsed Time: 0.0900731086730957\n",
            "Episode 285/10000, Total Reward: -102.4334406829115, Elapsed Time: 0.08158373832702637\n",
            "Episode 286/10000, Total Reward: -200.03828684398005, Elapsed Time: 2.9585983753204346\n",
            "Episode 287/10000, Total Reward: -103.16126447245702, Elapsed Time: 0.06083512306213379\n",
            "Episode 288/10000, Total Reward: -104.65211223755429, Elapsed Time: 0.05720806121826172\n",
            "Episode 289/10000, Total Reward: -98.72487602918831, Elapsed Time: 0.06456851959228516\n",
            "Episode 290/10000, Total Reward: -102.46242069858151, Elapsed Time: 0.0530240535736084\n",
            "Episode 291/10000, Total Reward: -184.03444739155128, Elapsed Time: 0.9008281230926514\n",
            "Episode 292/10000, Total Reward: -200.10472519366877, Elapsed Time: 2.4463717937469482\n",
            "Episode 293/10000, Total Reward: -118.07295376422704, Elapsed Time: 0.049504995346069336\n",
            "Episode 294/10000, Total Reward: -141.1701786991669, Elapsed Time: 0.30493736267089844\n",
            "Episode 295/10000, Total Reward: -173.74303881643937, Elapsed Time: 0.6956181526184082\n",
            "Episode 296/10000, Total Reward: -99.5363135815952, Elapsed Time: 0.06871914863586426\n",
            "Episode 297/10000, Total Reward: -102.52241762721825, Elapsed Time: 0.058563232421875\n",
            "Episode 298/10000, Total Reward: -118.49613044199161, Elapsed Time: 0.08495545387268066\n",
            "Episode 299/10000, Total Reward: -117.89422583355021, Elapsed Time: 0.09376788139343262\n",
            "Episode 300/10000, Total Reward: -133.46272288950695, Elapsed Time: 0.1037895679473877\n",
            "Episode 301/10000, Total Reward: -117.75017642203048, Elapsed Time: 0.05858778953552246\n",
            "Episode 302/10000, Total Reward: -120.27594192931514, Elapsed Time: 0.09002900123596191\n",
            "Episode 303/10000, Total Reward: -102.34724982489794, Elapsed Time: 0.057445526123046875\n",
            "Episode 304/10000, Total Reward: -127.5401080108476, Elapsed Time: 0.11271405220031738\n",
            "Episode 305/10000, Total Reward: -101.11117955145315, Elapsed Time: 0.07651281356811523\n",
            "Episode 306/10000, Total Reward: -124.16388245523646, Elapsed Time: 0.08429098129272461\n",
            "Episode 307/10000, Total Reward: -101.4744460831852, Elapsed Time: 0.06414413452148438\n",
            "Episode 308/10000, Total Reward: -113.67057771166961, Elapsed Time: 0.06887125968933105\n",
            "Episode 309/10000, Total Reward: -154.75819536534857, Elapsed Time: 0.34929752349853516\n",
            "Episode 310/10000, Total Reward: -101.35712178382973, Elapsed Time: 0.09006738662719727\n",
            "Episode 311/10000, Total Reward: -100.6270758696607, Elapsed Time: 0.0693521499633789\n",
            "Episode 312/10000, Total Reward: -146.06301387349393, Elapsed Time: 0.2949395179748535\n",
            "Episode 313/10000, Total Reward: -110.88466470223827, Elapsed Time: 0.044748544692993164\n",
            "Episode 314/10000, Total Reward: -125.27056653739773, Elapsed Time: 0.08321142196655273\n",
            "Episode 315/10000, Total Reward: -101.71300259961833, Elapsed Time: 0.03974604606628418\n",
            "Episode 316/10000, Total Reward: -101.26934068896571, Elapsed Time: 0.08291792869567871\n",
            "Episode 317/10000, Total Reward: -102.91378907461564, Elapsed Time: 0.03909921646118164\n",
            "Episode 318/10000, Total Reward: -117.46623471562614, Elapsed Time: 0.04395413398742676\n",
            "Episode 319/10000, Total Reward: -105.92383556464551, Elapsed Time: 0.07305407524108887\n",
            "Episode 320/10000, Total Reward: -117.78700720684441, Elapsed Time: 0.05516195297241211\n",
            "Episode 321/10000, Total Reward: -99.99568211712291, Elapsed Time: 0.057891845703125\n",
            "Episode 322/10000, Total Reward: -102.75298995024984, Elapsed Time: 0.056096553802490234\n",
            "Episode 323/10000, Total Reward: -104.89251831864628, Elapsed Time: 0.06052708625793457\n",
            "Episode 324/10000, Total Reward: -99.2578399574846, Elapsed Time: 0.07290506362915039\n",
            "Episode 325/10000, Total Reward: -104.90849712225707, Elapsed Time: 0.06439089775085449\n",
            "Episode 326/10000, Total Reward: -101.58901630631163, Elapsed Time: 0.06559324264526367\n",
            "Episode 327/10000, Total Reward: -100.74917856770294, Elapsed Time: 0.05984187126159668\n",
            "Episode 328/10000, Total Reward: -103.9659440125073, Elapsed Time: 0.046315908432006836\n",
            "Episode 329/10000, Total Reward: -104.5142777573465, Elapsed Time: 0.07151579856872559\n",
            "Episode 330/10000, Total Reward: -104.36041898205045, Elapsed Time: 0.09348106384277344\n",
            "Episode 331/10000, Total Reward: -115.41659936087915, Elapsed Time: 0.05998682975769043\n",
            "Episode 332/10000, Total Reward: -105.11651483849089, Elapsed Time: 0.07032132148742676\n",
            "Episode 333/10000, Total Reward: -102.44862557694688, Elapsed Time: 0.04965782165527344\n",
            "Episode 334/10000, Total Reward: -103.97563406031989, Elapsed Time: 0.05375981330871582\n",
            "Episode 335/10000, Total Reward: -103.3821555040306, Elapsed Time: 0.08553147315979004\n",
            "Episode 336/10000, Total Reward: -99.46688866753193, Elapsed Time: 0.09729385375976562\n",
            "Episode 337/10000, Total Reward: -116.44707151319633, Elapsed Time: 0.09497356414794922\n",
            "Episode 338/10000, Total Reward: -122.97802484180703, Elapsed Time: 0.12612605094909668\n",
            "Episode 339/10000, Total Reward: -101.60612936629181, Elapsed Time: 0.0796365737915039\n",
            "Episode 340/10000, Total Reward: -104.98961426843455, Elapsed Time: 0.09744715690612793\n",
            "Episode 341/10000, Total Reward: -186.762809493583, Elapsed Time: 1.8320274353027344\n",
            "Episode 342/10000, Total Reward: -100.94476788607861, Elapsed Time: 0.08393120765686035\n",
            "Episode 343/10000, Total Reward: -99.7855812733362, Elapsed Time: 0.09896397590637207\n",
            "Episode 344/10000, Total Reward: -103.83014201040181, Elapsed Time: 0.10281848907470703\n",
            "Episode 345/10000, Total Reward: -99.08159717452588, Elapsed Time: 0.1015768051147461\n",
            "Episode 346/10000, Total Reward: -117.956780358109, Elapsed Time: 0.15048575401306152\n",
            "Episode 347/10000, Total Reward: -102.70645542572215, Elapsed Time: 0.11859846115112305\n",
            "Episode 348/10000, Total Reward: -99.10817449108015, Elapsed Time: 0.12739205360412598\n",
            "Episode 349/10000, Total Reward: -105.0750789786792, Elapsed Time: 0.09479856491088867\n",
            "Episode 350/10000, Total Reward: -118.71869337783133, Elapsed Time: 0.08378934860229492\n",
            "Episode 351/10000, Total Reward: -117.45271238632873, Elapsed Time: 0.09951448440551758\n",
            "Episode 352/10000, Total Reward: -103.98538170677125, Elapsed Time: 0.09728121757507324\n",
            "Episode 353/10000, Total Reward: -200.0500892000047, Elapsed Time: 2.911864995956421\n",
            "Episode 354/10000, Total Reward: -122.5655063807162, Elapsed Time: 0.08730196952819824\n",
            "Episode 355/10000, Total Reward: -100.06918301426073, Elapsed Time: 0.06066393852233887\n",
            "Episode 356/10000, Total Reward: -118.96772579523089, Elapsed Time: 0.06786346435546875\n",
            "Episode 357/10000, Total Reward: -100.79632659053529, Elapsed Time: 0.05878734588623047\n",
            "Episode 358/10000, Total Reward: -123.09025644807119, Elapsed Time: 0.0910787582397461\n",
            "Episode 359/10000, Total Reward: -101.45851777648281, Elapsed Time: 0.08272266387939453\n",
            "Episode 360/10000, Total Reward: -99.9574222751343, Elapsed Time: 0.07402157783508301\n",
            "Episode 361/10000, Total Reward: -110.48574182572402, Elapsed Time: 0.05591630935668945\n",
            "Episode 362/10000, Total Reward: -100.45274526587339, Elapsed Time: 0.06211662292480469\n",
            "Episode 363/10000, Total Reward: -132.06106152318617, Elapsed Time: 0.10797858238220215\n",
            "Episode 364/10000, Total Reward: -102.66770719828482, Elapsed Time: 0.06698894500732422\n",
            "Episode 365/10000, Total Reward: -105.45283157398141, Elapsed Time: 0.06293416023254395\n",
            "Episode 366/10000, Total Reward: -101.76120763763363, Elapsed Time: 0.07604193687438965\n",
            "Episode 367/10000, Total Reward: -103.60516635677094, Elapsed Time: 0.05377936363220215\n",
            "Episode 368/10000, Total Reward: -289.9954507178682, Elapsed Time: 2.208604335784912\n",
            "Episode 369/10000, Total Reward: -200.07975091096827, Elapsed Time: 2.604170799255371\n",
            "Episode 370/10000, Total Reward: -200.12367679420467, Elapsed Time: 2.832853078842163\n",
            "Episode 371/10000, Total Reward: -200.0287315959125, Elapsed Time: 62.96988868713379\n",
            "Episode 372/10000, Total Reward: -99.9436197952977, Elapsed Time: 0.06386160850524902\n",
            "Episode 373/10000, Total Reward: -104.78831600395193, Elapsed Time: 0.040636301040649414\n",
            "Episode 374/10000, Total Reward: -133.51659591494885, Elapsed Time: 0.10788512229919434\n",
            "Episode 375/10000, Total Reward: -103.01274429858972, Elapsed Time: 0.0567326545715332\n",
            "Episode 376/10000, Total Reward: -103.52075169585025, Elapsed Time: 0.03729677200317383\n",
            "Episode 377/10000, Total Reward: -142.83361151889142, Elapsed Time: 0.18590450286865234\n",
            "Episode 378/10000, Total Reward: -99.92361833674286, Elapsed Time: 0.055878400802612305\n",
            "Episode 379/10000, Total Reward: -106.07421708234722, Elapsed Time: 0.03624677658081055\n",
            "Episode 380/10000, Total Reward: -99.72820808767155, Elapsed Time: 0.058625221252441406\n",
            "Episode 381/10000, Total Reward: -101.6342018873596, Elapsed Time: 0.0563969612121582\n",
            "Episode 382/10000, Total Reward: -101.75736090707208, Elapsed Time: 0.07191181182861328\n",
            "Episode 383/10000, Total Reward: -121.85661959382408, Elapsed Time: 0.05899858474731445\n",
            "Episode 384/10000, Total Reward: -104.47847965437373, Elapsed Time: 0.09844112396240234\n",
            "Episode 385/10000, Total Reward: -114.51483712322003, Elapsed Time: 0.044078826904296875\n",
            "Episode 386/10000, Total Reward: -114.71519615220416, Elapsed Time: 0.048090457916259766\n",
            "Episode 387/10000, Total Reward: -104.2153568870008, Elapsed Time: 0.05040311813354492\n",
            "Episode 388/10000, Total Reward: -101.96645436417312, Elapsed Time: 0.08558297157287598\n",
            "Episode 389/10000, Total Reward: -102.34826096778723, Elapsed Time: 0.04985308647155762\n",
            "Episode 390/10000, Total Reward: -213.14860633223617, Elapsed Time: 1.0188219547271729\n",
            "Episode 391/10000, Total Reward: -103.8222030446245, Elapsed Time: 0.07251286506652832\n",
            "Episode 392/10000, Total Reward: -99.73158398581768, Elapsed Time: 0.06198239326477051\n",
            "Episode 393/10000, Total Reward: -100.33519308245033, Elapsed Time: 0.061873674392700195\n",
            "Episode 394/10000, Total Reward: -102.85729276492994, Elapsed Time: 0.06290817260742188\n",
            "Episode 395/10000, Total Reward: -101.38630159898288, Elapsed Time: 0.06536722183227539\n",
            "Episode 396/10000, Total Reward: -99.96329120270859, Elapsed Time: 0.053933143615722656\n",
            "Episode 397/10000, Total Reward: -106.70148706054563, Elapsed Time: 0.04852151870727539\n",
            "Episode 398/10000, Total Reward: -100.12671648454827, Elapsed Time: 0.06338834762573242\n",
            "Episode 399/10000, Total Reward: -119.98290624217528, Elapsed Time: 0.05547213554382324\n",
            "Episode 400/10000, Total Reward: -105.52597013793016, Elapsed Time: 0.06979608535766602\n",
            "Episode 401/10000, Total Reward: -101.43164826842758, Elapsed Time: 0.05089521408081055\n",
            "Episode 402/10000, Total Reward: -100.81149421627286, Elapsed Time: 0.06501483917236328\n",
            "Episode 403/10000, Total Reward: -122.37293570291884, Elapsed Time: 0.09844064712524414\n",
            "Episode 404/10000, Total Reward: -123.97542640259465, Elapsed Time: 0.08481168746948242\n",
            "Episode 405/10000, Total Reward: -102.26776408828609, Elapsed Time: 0.06984996795654297\n",
            "Episode 406/10000, Total Reward: -102.13918967579218, Elapsed Time: 127.19921851158142\n"
          ]
        }
      ],
      "source": [
        "if mode == 'uniform':\n",
        "    q_learning = QTableLearning(env, mode=mode)  # Initialize Q-learning with uniform discretization\n",
        "elif mode == 'range':\n",
        "    q_learning = QTableLearning(env, obs_buckets=1, mode=mode)  # Initialize Q-learning with range-based discretization\n",
        "if mode == 'uniform' or mode == 'range':\n",
        "    file_name = f'qtable_{mode}.dill'\n",
        "\n",
        "    qtable, rewards, elapsed_times = q_learning.learn()\n",
        "\n",
        "    # Save the Q-table \n",
        "    with open(f'qtable_{mode}.dill', 'wb') as f:\n",
        "        dill.dump(qtable, f)\n",
        "\n",
        "    # Plot rewards \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(rewards, label='Total Reward')\n",
        "    if len(rewards) >= window_size:\n",
        "        plt.plot(range(window_size, len(rewards) + 1), moving_average(rewards), label='Moving Average (100 episodes)')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.title(f'Reward Evolution ({mode} discretization)')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'Rewards Evolutions {mode}.png')\n",
        "\n",
        "    # Plot elapsed times\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(elapsed_times, label=f'Elapsed Time ({mode})')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Elapsed Time (s)')\n",
        "    plt.title(f'Elapsed Time per Episode ({mode} discretization)')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'Elapsed Times {mode}.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kzd08flNC0Ob"
      },
      "outputs": [],
      "source": [
        "import dill\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def load_qtable(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        qtable = dill.load(f)\n",
        "    return qtable\n",
        "\n",
        "\n",
        "mode = 'uniform'\n",
        "act_buckets = 11\n",
        "obs_buckets = 11\n",
        "\n",
        "if mode == range:\n",
        "    obs_buckets = 1\n",
        "\n",
        "filename = f'qtable_{mode}.dill'\n",
        "qtable = load_qtable(filename)\n",
        "env = gym.make(\"BipedalWalker-v3\", render_mode = 'human')\n",
        "\n",
        "\n",
        "def test_qtable(env, qtable, episodes=100, render=True):\n",
        "    rewards = []  \n",
        "    for episode in range(1, episodes + 1):\n",
        "        total_reward = 0\n",
        "        state = env.reset()[0]\n",
        "        state = discretizeState(state)\n",
        "\n",
        "        while True:\n",
        "            action = np.argmax(qtable[state])\n",
        "            continuous_action = undiscretizeAction(action)\n",
        "            next_state, reward, done, _, _ = env.step(continuous_action)\n",
        "\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "            next_state = discretizeState(next_state)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "        print(f\"Test Episode {episode}/{episodes}, Total Reward: {total_reward}\")\n",
        "\n",
        "    env.close()\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def discretizeState(state):\n",
        "    \"\"\"\n",
        "    Discretize the continuous observation state into discrete buckets based on the mode.\n",
        "\n",
        "    Parameters:\n",
        "    state (np.array): Continuous observation state.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Discretized state.\n",
        "    \"\"\"\n",
        "    if mode == 'uniform':\n",
        "        discrete_state = np.round((state - env.observation_space.low) / (env.observation_space.high - env.observation_space.low) * (obs_buckets - 1)).astype(int)\n",
        "    elif mode == 'range':\n",
        "        discrete_state = []\n",
        "        for i, val in enumerate(state):\n",
        "            # compute the number of buckets for each dimension\n",
        "            bucket_size = (env.observation_space.high[i] - env.observation_space.low[i]) * obs_buckets\n",
        "            # make sure that there is at least one bucket for each dimension\n",
        "            bucket_size = max(1, int(bucket_size))\n",
        "            # discretize the dimension\n",
        "            discrete_val = round((val - env.observation_space.low[i]) / (env.observation_space.high[i] - env.observation_space.low[i]) * (bucket_size - 1))\n",
        "            discrete_state.append(discrete_val)\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid mode '{mode}'. Supported modes are 'uniform' and 'range'.\")\n",
        "\n",
        "    return tuple(discrete_state)\n",
        "\n",
        "def discretizeAction(action):\n",
        "    \"\"\"\n",
        "    Discretize the continuous action into discrete buckets.\n",
        "\n",
        "    Parameters:\n",
        "    action (np.array): Continuous action.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Discretized action.\n",
        "    \"\"\"\n",
        "    discrete_action = np.round((action - env.action_space.low) / (env.action_space.high - env.action_space.low) * (act_buckets - 1)).astype(int)\n",
        "    return tuple(discrete_action)\n",
        "\n",
        "def undiscretizeAction(action):\n",
        "    \"\"\"\n",
        "    Convert a discrete action back into a continuous action.\n",
        "\n",
        "    Parameters:\n",
        "    action (tuple): Discretized action.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Continuous action.\n",
        "    \"\"\"\n",
        "    action = (np.array(action) / (act_buckets - 1)) * (env.action_space.high - env.action_space.low) + env.action_space.low\n",
        "    return tuple(action)\n",
        "\n",
        "\n",
        "test_rewards = test_qtable(env, qtable, episodes=100, render=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(test_rewards, label='Total Reward')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title(f'Rewards of {mode} mode (Testing with Loaded Q-table)')\n",
        "plt.legend()\n",
        "plt.savefig(f'Rewards {mode}.png')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
